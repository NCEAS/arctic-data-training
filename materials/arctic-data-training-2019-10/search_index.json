[
["index.html", "Arctic Data Center Training 1 Data Science Training for the Arctic Data Center 1.1 Schedule", " Arctic Data Center Training October 7-11, 2019 1 Data Science Training for the Arctic Data Center The Arctic Data Center conducts training in data science and management, both of which are critical skills for stewardship of data, software, and other products of research that are preserved at the Arctic Data Center. This work is licensed under a Creative Commons Attribution 4.0 International License. Citation: Jones, Matthew B., Amber E. Budden, Bryce Mecum, Jeanette Clark, Julien Brun, and Julie Lowndes. 2019. Data Science Training for Arctic Researchers. Arctic Data Center. doi:10.18739/A24746R2N 1.1 Schedule 1.1.1 Code of Conduct Arctic Data Center Code of Conduct 1.1.1.1 Acknowledgements Work on this package was supported by: NSF award #1546024 to M. B. Jones, S. Baker-Yeboah, A. Budden, J. Dozier, and M. Schildhauer Additional support was provided for working group collaboration by the National Center for Ecological Analysis and Synthesis, a Center funded by the University of California, Santa Barbara, and the State of California. "],
["introduction-to-the-arctic-data-center-and-nsf-standards-and-policies.html", "2 Introduction to the Arctic Data Center and NSF Standards and Policies 2.1 Learning Objectives 2.2 Materials", " 2 Introduction to the Arctic Data Center and NSF Standards and Policies 2.1 Learning Objectives In this lesson, you will learn: About the mission and structure of the Arctic Data Center How the Arctic Data Center supports the research community About data policies from the NSF Arctic program 2.2 Materials Download slides: Arctic Data Center Overview and NSF Policies "],
["best-practices-data-and-metadata.html", "3 Best Practices: Data and Metadata 3.1 Learning Objectives 3.2 Preserving computational workflows 3.3 Best Practices: Overview 3.4 Organizing Data: Best Practices", " 3 Best Practices: Data and Metadata 3.1 Learning Objectives In this lesson, you will learn: Why preserving computational workflows is important How to acheive practical reproducibility What are some best practices for data and metadata management Download slides: Best Practices: Data and Metadata 3.2 Preserving computational workflows Preservation enables: Understanding Evaluation Reuse All for Future You! And your collaborators and colleagues across disciplines. Figure 3.1: Scientific products that need to be preserved from computational workflows include data, software, metadata, and other products like graphs and figures. While the Arctic Data Center and similar repositories do focus on preserving data, we really set our sights much more broadly on preserving entire computational workflows that are instrumental to advances in science. A computational workflow represents the sequence of computational tasks that are performed from raw data acquisition through data quality control, integration, analysis, modeling, and visualization. Figure 3.2: Computational steps can be organized as a workflow streaming raw data through to derived products. In addition, these workflows are often not executed all at once, but rather are divided into multiple workflows, earch with its own purpose. For example, a data acquistion and cleaning workflow often creates a derived and integrated data product that is then picked up and used by multiple downstream analytical workflows that produce specific scientific findings. These workflows can each be archived as distinct data packages, with the output of the first workflow becoming the input of the second and subsequent workflows. Figure 3.3: Computational workflows can be archived and preserved in multiple dat apackages that are linked by their shared components, in this case an intermediate data file. 3.3 Best Practices: Overview Who Must Submit? Organizing Data File Formats Large Data Packages Metadata Data Identifiers Provenance Licensing and Distribution 3.4 Organizing Data: Best Practices Both (Borer et al. (2009)) and (White et al. (2013)) provide some practical guidelines for organizing and structuring data. Critical aspects of their recommendations include: Write scripts for all data manipulation Uncorrected raw data file Document processing in scripts Design to add rows, not columns Each column one variable Each row one observation Use nonproprietary file formats Descriptive names, no spaces Header line References "],
["thinking-preferences.html", "4 Thinking preferences 4.1 Learning Objectives 4.2 Resources", " 4 Thinking preferences 4.1 Learning Objectives In this lesson, you will: Participate in an interactive group activity Learn about variation in thinking preferences 4.2 Resources Thinking preferences overview "],
["data-documentation-and-publishing.html", "5 Data Documentation and Publishing 5.1 Learning Objectives 5.2 Data sharing and preservation 5.3 Data repositories: built for data (and code) 5.4 Metadata 5.5 Structure of a data package 5.6 DataONE Federation 5.7 Publishing data from the web", " 5 Data Documentation and Publishing 5.1 Learning Objectives In this lesson, you will learn: About open data archives, especially the Arctic Data Center What science metadata are and how they can be used How data and code can be documented and published in open data archives Web-based submission 5.2 Data sharing and preservation 5.3 Data repositories: built for data (and code) GitHub is not an archival location Dedicated data repositories: KNB, Arctic Data Center, tDAR, EDI, Zenodo Rich metadata Archival in their mission Certification for repositories: https://www.coretrustseal.org/ Data papers, e.g., Scientific Data List of data repositories: http://re3data.org 5.4 Metadata Metadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where they were collected, and why they were collected. For consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the metadata for a sockeye salmon dataset: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; That same metadata document can be converted to HTML format and displayed in a much more readable form on the web: https://knb.ecoinformatics.org/#view/doi:10.5063/F1F18WN4 And as you can see, the whole dataset or its components can be downloaded and reused. Also note that the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data. 5.5 Structure of a data package Note that the dataset above lists a collection of files that are contained within the dataset. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document. These data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4,and each of the individual files would have their own identifiers as well. 5.6 DataONE Federation DataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses. DataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data from the dozens of members of DataONE, rather than visiting each of the currently 43 repositories one at a time. 5.7 Publishing data from the web Each data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the Arctic Data Center, we provide some easy to use web forms for editing and submitting a data package. Let’s walk through a web submission to see what you might expect. 5.7.1 Download the data to be used for the tutorial I’ve already uploaded the test data package, and so you can access the data here: https://demo.arcticdata.io/#view/urn:uuid:0702cc63-4483-4af4-a218-531ccc59069f Grab both CSV files, and the R script, and store them in a convenient folder. 5.7.2 Login via ORCID We will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button. When you sign in, you will be redirected to orcid.org, where you can either provide your existing ORCID credentials, or create a new account. ORCID provides multiple ways to login, including using your email address, institutional logins from many universities, and logins from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login. 5.7.3 Create and submit the dataset After signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. 5.7.3.1 Click Add Files to choose the data files for your package You can select multiple files at a time to efficiently upload many files. The files will upload showing a progress indicator. You can continue editing metadata while they upload. 5.7.3.2 Enter Overview information This includes a descriptive title, abstract, and keywords. And you also must enter a funding award number and choose a license. The funding field will search for an NSF award identifier based on words in its title or the number itself. 5.7.3.3 People Information Information about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people: Creators - all the people who should be in the citation for the dataset Contacts - one is required, but defaults to the first Creator if omitted Principal Investigators and any other that are relevant For each, please strive to provide their ORCID identifier, which helps link this dataset to their other scholarly works. 5.7.3.4 Location Information The geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values for West longitudes. The editor allows you to enter multiple locations, which you should do if you had noncontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that spatial search will be more precise. Note that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global: Mission Canyon, Santa Barbara, California, USA 5.7.3.5 Temporal Information Add the temporal coverage of the data, which represents the time period to which data apply. Again, use multiple date ranges if your sampling was discontinuous. 5.7.3.6 Methods Methods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, include details of sampling methods, experimental design, quality assurance procedures, and computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data. 5.7.3.7 Save a first version with Submit When finished, click the Submit Dataset button at the bottom. If there are errors or missing fields, they will be highlighted. Correct those, and then try submitting again. When you are successful, you should see a large green banner with a link to the current dataset view. Click the X to close that banner, if you want to continue editing metadata. Success! 5.7.4 File and variable level metadata The final major section of metadata concerns the structure and contents of your data files. In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure. For example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information. The Attributes tab is where you enter variable (aka attribute) information, including: variable name (for programs) variable label (for display) - variable definition (be specific) - type of measurement - units &amp; code definitions You’ll need to add these definitions for every variable (column) in the file. When done, click Done. Now the list of data files will show a green checkbox indicating that you have full described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes. After you get the big green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes. 5.7.5 Add workflow provenance Understanding the relationships between files in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, that are often then used in analysis and visualization code to produce final outputs. In DataONE, we support structured descriptions of these relationships, so one can see the flow of data from raw data to derived to outputs. You add provenance by navigating to the data table descriptions, and selecting the Add buttons to link the data and scripts that were used in your computational workflow. On the left side, select the Add circle to add an input data source to the filteredSpecies.csv file. This starts building the provenance graph to explain the origin and history of each data object. The linkage to the source dataset should appear. Then you can add the link to the source code that handled the conversion between the data files by clicking on Add arrow and selecting the R script: The diagram now shows the relationships among the data files and the R script, so click Submit to save another version of the package. Et voilà! A beatifully preserved data package! "],
["rstudio-and-gitgithub-setup-and-motivation.html", "6 RStudio and Git/GitHub Setup and Motivation 6.1 Learning Objectives 6.2 Reproducible Research 6.3 Why use git? 6.4 Checking the RStudio environment 6.5 Setting up git", " 6 RStudio and Git/GitHub Setup and Motivation 6.1 Learning Objectives In this lesson, you will learn: What computational reproducibility is and why it is useful How version control can increase computational reproducibility How to check to make sure your RStudio environment is set up properly for analysis How to set up git 6.2 Reproducible Research Reproducibility is the hallmark of science, which is based on empirical observations coupled with explanatory models. While reproducibility encompasses the full science lifecycle, and includes issues such as methodological consistency and treatment of bias, in this course we will focus on computational reproducibility: the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions. 6.2.1 What is needed for computational reproducibility? The first step towards addressing these issues is to be able to evaluate the data, analyses, and models on which conclusions are drawn. Under current practice, this can be difficult because data are typically unavailable, the method sections of papers do not detail the computational approaches used, and analyses and models are often conducted in graphical programs, or, when scripted analyses are employed, the code is not available. And yet, this is easily remedied. Researchers can achieve computational reproducibility through open science approaches, including straightforward steps for archiving data and code openly along with the scientific workflows describing the provenance of scientific results (e.g., Hampton et al. (2015), Munafò et al. (2017)). 6.2.2 Conceptualizing workflows Scientific workflows encapsulate all of the steps from data acquisition, cleaning, transformation, integration, analysis, and visualization. Workflows can range in detail from simple flowcharts to fully executable scripts. R scripts and python scripts are a textual form of a workflow, and when researchers publish specific versions of the scripts and data used in an analysis, it becomes far easier to repeat their computations and understand the provenance of their conclusions. 6.3 Why use git? 6.3.1 The problem with filenames Every file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when problems are discovered. Data files get combined together, then errors are fixed, and then they are split and combined again. In the course of a single analysis, one can expect thousands of changes to files. And yet, all we use to track this are simplistic filenames. You might think there is a better way, and you’d be right: version control. Version control systems help you track all of the changes to your files, without the spaghetti mess that ensues from simple file renaming. In version control systems like git, the system tracks not just the name of the file, but also its contents, so that when contents change, it can tell you which pieces went where. It tracks which version of a file a new version came from. So its easy to draw a graph showing all of the versions of a file, like this one: Version control systems assign an identifier to every version of every file, and track their relationships. They also allow branches in those versions, and merging those branches back into the main line of work. They also support having multiple copies on multiple computers for backup, and for collaboration. And finally, they let you tag particular versions, such that it is easy to return to a set of files exactly as they were when you tagged them. For example, the exact versions of data, code, and narrative that were used when a manuscript was submitted might be R2 in the graph above. 6.4 Checking the RStudio environment 6.4.1 R Version We will use R version 3.6.1, which you can download and install from CRAN. To check your version, run this in your RStudio console: R.version$version.string 6.4.1.1 Updating R If you already have R installed, but need to update, and don’t want to lose your packages, these two R functions can help you. The first will save all of your packages to a file. The second loads the packages from the file and installs packages that are missing. Save this script to a file (eg package_update.R). #&#39; Save R packages to a file. Useful when updating R version #&#39; #&#39; @param path path to rda file to save packages to. eg: installed_old.rda save_packages &lt;- function(path){ tmp &lt;- installed.packages() installedpkgs &lt;- as.vector(tmp[is.na(tmp[,&quot;Priority&quot;]), 1]) save(installedpkgs, file = path) } #&#39; Update packages from a file. Useful when updating R version #&#39; #&#39; @param path path to rda file where packages were saved update_packages &lt;- function(path){ tmp &lt;- new.env() installedpkgs &lt;- load(file = path, envir = tmp) installedpkgs &lt;- tmp[[ls(tmp)[1]]] tmp &lt;- installed.packages() installedpkgs.new &lt;- as.vector(tmp[is.na(tmp[,&quot;Priority&quot;]), 1]) missing &lt;- setdiff(installedpkgs, installedpkgs.new) install.packages(missing) update.packages() } Source the file that you saved above (eg: source(package_update.R)). Then, run the save_packages function. save_packages(&quot;installed_old.rda&quot;) Then quit R, go to CRAN, and install the latest version of R. Source the R script that you saved above again, and then run: update_packages(&quot;installed_old.rda&quot;) This should install all of your R packages that you had before you upgraded. 6.4.2 RStudio Version We will be using RStudio version 1.2.500 or later, which you can download and install here To check your RStudio version, run the following in your RStudio console: RStudio.Version()$version If the output of this does not say 1.2.500, you should update your RStudio. Do this by selecting Help -&gt; Check for Updates and follow the prompts. 6.4.3 Package installation Run the following lines to check that all of the packages we need for the training are installed on your computer. packages &lt;- c(&quot;dataone&quot;, &quot;datapack&quot;, &quot;devtools&quot;, &quot;dplyr&quot;, &quot;DT&quot;, &quot;EML&quot;, &quot;ggplot2&quot;, &quot;leaflet&quot;, &quot;readxl&quot;, &quot;roxygen2&quot;, &quot;tidyr&quot;, &quot;EML&quot;, &quot;shinyjs&quot;, &quot;sf&quot;) for (package in packages) { if (!(package %in% installed.packages())) { install.packages(package) } } rm(packages) #remove variables from workspace If you haven’t installed all of the packages, this will automatically start installing them. If they are installed, it won’t do anything. Next, create a new R Markdown (File -&gt; New File -&gt; R Markdown). If you have never made an R Markdown document before, a dialog box will pop up asking if you wish to install the required packages. Click yes. 6.5 Setting up git If you haven’t already, go to github.com and create an account. If you haven’t downloaded git already, you can download it here. Before using git, you need to tell it who you are, also known as setting the global options. The only way to do this is through the command line. Newer versions of RStudio have a nice feature where you can open a terminal window in your RStudio session. Do this by selecting Tools -&gt; Terminal -&gt; New Terminal. A terminal tab should now be open where your console usually is. To set the global options, type the following into the command prompt, with your actual name, and press enter: git config --global user.name &quot;Your Name&quot; Next, enter the following line, with the email address you used when you created your account on github.com: git config --global user.email &quot;yourEmail@emaildomain.com&quot; Note that these lines need to be run one at a time. Finally, check to make sure everything looks correct by entering this line, which will return the options that you have set. git config --global --list 6.5.1 Note for Windows Users If you get “command not found” (or similar) when you try these steps through the RStudio terminal tab, you may need to set the type of terminal that gets launched by RStudio. Under some git install senerios, the git executable may not be available to the default terminal type. References "],
["introduction-to-r-and-rmarkdown.html", "7 Introduction to R and RMarkdown 7.1 Learning Objectives 7.2 Introduction and Motivation 7.3 R at the console 7.4 RMarkdown 7.5 R functions, help pages 7.6 Using data.frames 7.7 Troubleshooting 7.8 Literate Analysis", " 7 Introduction to R and RMarkdown 7.1 Learning Objectives In this lesson we will: get oriented to the RStudio interface work with R in the console explore RMarkdown be introduced to built-in R functions learn to use the help pages 7.2 Introduction and Motivation There is a vibrant community out there that is collectively developing increasingly easy to use and powerful open source programming tools. The changing landscape of programming is making learning how to code easier than it ever has been. Incorporating programming into analysis workflows not only makes science more efficient, but also more computationally reproducible. In this course, we will use the programming language R, and the accompanying integrated development environment (IDE) RStudio. R is a great language to learn for data-oriented programming because it is widely adopted, user-friendly, and (most importantly) open source! So what is the difference between R and RStudio? Here is an analogy to start us off. If you were a chef, R is a knife. You have food to prepare, and the knife is one of the tools that you’ll use to accomplish your task. And if R were a knife, RStudio is the kitchen. RStudio provides a place to do your work! Other tools, communication, community, it makes your life as a chef easier. RStudio makes your life as a researcher easier by bringing together other tools you need to do your work efficiently - like a file browser, data viewer, help pages, terminal, community, support, the list goes on. So it’s not just the infrastructure (the user interface or IDE), although it is a great way to learn and interact with your variables, files, and interact directly with git. It’s also data science philosophy, R packages, community, and more. So although you can prepare food without a kitchen and we could learn R without RStudio, that’s not what we’re going to do. We are going to take advantage of the great RStudio support, and learn R and RStudio together. Something else to start us off is to mention that you are learning a new language here. It’s an ongoing process, it takes time, you’ll make mistakes, it can be frustrating, but it will be overwhelmingly awesome in the long run. We all speak at least one language; it’s a similar process, really. And no matter how fluent you are, you’ll always be learning, you’ll be trying things in new contexts, learning words that mean the same as others, etc, just like everybody else. And just like any form of communication, there will be miscommunications that can be frustrating, but hands down we are all better off because of it. While language is a familiar concept, programming languages are in a different context from spoken languages, but you will get to know this context with time. For example: you have a concept that there is a first meal of the day, and there is a name for that: in English it’s “breakfast”. So if you’re learning Spanish, you could expect there is a word for this concept of a first meal. (And you’d be right: ‘desayuno’). We will get you to expect that programming languages also have words (called functions in R) for concepts as well. You’ll soon expect that there is a way to order values numerically. Or alphabetically. Or search for patterns in text. Or calculate the median. Or reorganize columns to rows. Or subset exactly what you want. We will get you increase your expectations and learn to ask and find what you’re looking for. 7.2.1 Resources This lesson is a combination of excellent lessons by others. Huge thanks to Julie Lowndes for writing most of this content and letting us build on her material, which in turn was built on Jenny Bryan’s materials. I definitely recommend reading through the original lessons and using them as reference: Julie Lowndes’ Data Science Training for the Ocean Health Index R, RStudio, RMarkdown Programming in R Jenny Bryan’s lectures from STAT545 at UBC R basics, workspace and working directory, RStudio projects Basic care and feeding of data in R RStudio has great resources as well: webinars cheatsheets Finally, Hadley Wickham’s book R for Data Science is a great resource to get more in depth. -R for Data Science Other resources: LaTeX Equation Formatting Base R Cheatsheet RMarkdown Reference Guide MATLAB/R Translation Cheat Sheet 7.3 R at the console Launch RStudio/R. Notice the default panes: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) FYI: you can change the default location of the panes, among many other things: Customizing RStudio. An important first question: where are we? If you’ve just opened RStudio for the first time, you’ll be in your Home directory. This is noted by the ~/ at the top of the console. You can see too that the Files pane in the lower right shows what is in the Home directory where you are. You can navigate around within that Files pane and explore, but note that you won’t change where you are: even as you click through you’ll still be Home: ~/. OK let’s go into the Console, where we interact with the live R process. We use R to calculate things for us, so let’s do some simple math. 3*4 ## [1] 12 You can assign the value of that mathematic operation to a variable, or object, in R. You do this using the assignment operator, &lt;-. Make an assignment and then inspect the object you just created. x &lt;- 3 * 4 x ## [1] 12 In my head I hear, e.g., “x gets 12”. All R statements where you create objects – “assignments” – have this form: objectName &lt;- value. I’ll write it in the console with a hash #, which is the way R comments so it won’t be evaluated. ## objectName &lt;- value ## This is also how you write notes in your code to explain what you are doing. Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a convention for demarcating words in names. # i_use_snake_case # other.people.use.periods # evenOthersUseCamelCase Make an assignment this_is_a_really_long_name &lt;- 2.5 To inspect this variable, instead of typing it, we can press the up arrow key and call your command history, with the most recent commands first. Let’s do that, and then delete the assignment: this_is_a_really_long_name ## [1] 2.5 Another way to inspect this variable is to begin typing this_…and RStudio will automagically have suggested completions for you that you can select by hitting the tab key, then press return. One more: science_rocks &lt;- &quot;yes it does!&quot; You can see that we can assign an object to be a word, not a number. In R, this is called a “string”, and R knows it’s a word and not a number because it has quotes &quot; &quot;. You can work with strings in your data in R pretty easily, thanks to the stringr and tidytext packages. We won’t talk about strings very much specifically, but know that R can handle text, and it can work with text and numbers together. Strings and numbers lead us to an important concept in programming: that there are different “classes” or types of objects. An object is a variable, function, data structure, or method that you have written to your environment. You can see what objects you have loaded by looking in the “environment” pane in RStudio. The operations you can do with an object will depend on what type of object it is. This makes sense! Just like you wouldn’t do certain things with your car (like use it to eat soup), you won’t do certain operations with character objects (strings), for example. Try running the following line in your console: &quot;Hello world!&quot; * 3 What happened? Why? You may have noticed that when assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name: weight_kg &lt;- 55 # doesn&#39;t print anything (weight_kg &lt;- 55) # but putting parenthesis around the call prints the value of `weight_kg` ## [1] 55 weight_kg # and so does typing the name of the object ## [1] 55 Now that R has weight_kg in memory, we can do arithmetic with it. For instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg): 2.2 * weight_kg ## [1] 121 We can also change a variable’s value by assigning it a new one: weight_kg &lt;- 57.5 2.2 * weight_kg ## [1] 126.5 This means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a new variable, weight_lb: weight_lb &lt;- 2.2 * weight_kg and then change weight_kg to 100. weight_kg &lt;- 100 What do you think is the current content of the object weight_lb? 126.5 or 220? Why? You can also store more than one value in a single object. Storing a series of weights in a single object is a convenient way to perform the same operation on multiple values at the same time. One way to create such an object is the function c(), which stands for combine or concatenate. Here we will create a vector of weights in kilograms, and convert them to pounds, saving the weight in pounds as a new object. weight_kg &lt;- c(55, 25, 12) weight_kg ## [1] 55 25 12 weight_lb &lt;- weight_kg * 2.2 weight_lb ## [1] 121.0 55.0 26.4 7.3.1 Error messages are your friends Implicit contract with the computer/scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Pay attention to how you type. Remember that this is a language, not unsimilar to English! There are times you aren’t understood – it’s going to happen. There are different ways this can happen. Sometimes you’ll get an error. This is like someone saying ‘What?’ or ‘Pardon’? Error messages can also be more useful, like when they say ‘I didn’t understand this specific part of what you said, I was expecting something else’. That is a great type of error message. Error messages are your friend. Google them (copy-and-paste!) to figure out what they mean. And also know that there are errors that can creep in more subtly, without an error message right away, when you are giving information that is understood, but not in the way you meant. Like if I’m telling a story about tables and you’re picturing where you eat breakfast and I’m talking about data. This can leave me thinking I’ve gotten something across that the listener (or R) interpreted very differently. And as I continue telling my story you get more and more confused… So write clean code and check your work as you go to minimize these circumstances! 7.3.2 Logical operators and expressions A moment about logical operators and expressions. We can ask questions about the objects we just made. == means ‘is equal to’ != means ‘is not equal to’ &lt; means ` is less than’ &gt; means ` is greater than’ &lt;= means ` is less than or equal to’ &gt;= means ` is greater than or equal to’ weight_kg == 2 ## [1] FALSE FALSE FALSE weight_kg &gt;= 30 ## [1] TRUE FALSE FALSE weight_kg != 5 ## [1] TRUE TRUE TRUE Shortcuts You will make lots of assignments and the operator &lt;- is a pain to type. Don’t be lazy and use =, although it would work, because it will just sow confusion later. Instead, utilize RStudio’s keyboard shortcut: Alt + - (the minus sign). Notice that RStudio automagically surrounds &lt;- with spaces, which demonstrates a useful code formatting practice. Code is miserable to read on a good day. Give your eyes a break and use spaces. RStudio offers many handy keyboard shortcuts. Also, Alt+Shift+K brings up a keyboard shortcut reference card. 7.3.3 Clearing the environment Now look at the objects in your environment (workspace) – in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with a few different R commands: objects() ## [1] &quot;science_rocks&quot; &quot;this_is_a_really_long_name&quot; ## [3] &quot;weight_kg&quot; &quot;weight_lb&quot; ## [5] &quot;x&quot; ls() ## [1] &quot;science_rocks&quot; &quot;this_is_a_really_long_name&quot; ## [3] &quot;weight_kg&quot; &quot;weight_lb&quot; ## [5] &quot;x&quot; If you want to remove the object named weight_kg, you can do this: rm(weight_kg) To remove everything: rm(list = ls()) or click the broom in RStudio’s Environment pane. 7.4 RMarkdown Now that we know some basic R syntax, let’s learn a little about RMarkdown. You will drive yourself crazy (and fail to generate a reproducible workflow!) running code directly in the console. RMarkdown is really key for collaborative research, so we’re going to get started with it early and then use it for the rest of the course. An RMarkdown file will allow us to weave markdown text with chunks of R code to be evaluated and output content like tables and plots. File -&gt; New File -&gt; RMarkdown… -&gt; Document of output format HTML, OK. You can give it a Title like “My Project”. Then click OK. OK, first off: by opening a file, we are seeing the 4th pane of the RStudio console, which is essentially a text editor. This lets us organize our files within RStudio instead of having a bunch of different windows open. Let’s have a look at this file — it’s not blank; there is some initial text is already provided for you. Notice a few things about it: There are white and grey sections. R code is in grey sections, and other text is in white. Let’s go ahead and “Knit HTML” by clicking the blue yarn at the top of the RMarkdown file. When you first click this button, RStudio will prompt you to save this file. Create a new folder for it somewhere that you will be able to find again (such as your Desktop or Documents), and name that folder something you’ll remember (like arctic_training_files). What do you notice between the two? Notice how the grey R code chunks are surrounded by 3 backticks and {r LABEL}. These are evaluated and return the output text in the case of summary(cars) and the output plot in the case of plot(pressure). Notice how the code plot(pressure) is not shown in the HTML output because of the R code chunk option echo=FALSE. More details… This RMarkdown file has 2 different languages within it: R and Markdown. We don’t know that much R yet, but you can see that we are taking a summary of some data called ‘cars’, and then plotting. There’s a lot more to learn about R, and we’ll get into it for the next few days. The second language is Markdown. This is a formatting language for plain text, and there are only about 15 rules to know. Notice the syntax for: headers get rendered at multiple levels: #, ## bold: **word** There are some good cheatsheets to get you started, and here is one built into RStudio: Go to Help &gt; Markdown Quick Reference Important: note that the hashtag # is used differently in Markdown and in R: in R, a hashtag indicates a comment that will not be evaluated. You can use as many as you want: # is equivalent to ######. It’s just a matter of style. in Markdown, a hashtag indicates a level of a header. And the number you use matters: # is a “level one header”, meaning the biggest font and the top of the hierarchy. ### is a level three header, and will show up nested below the # and ## headers. Learn more: http://rmarkdown.rstudio.com/ 7.4.1 Your Turn In Markdown, Write some italic text, and make a numbered list. And add a few subheaders. Use the Markdown Quick Reference (in the menu bar: Help &gt; Markdown Quick Reference). Reknit your html file. 7.4.2 Code chunks OK. Now let’s practice with some of those commands. Create a new chunk in your RMarkdown first in one of these ways: click “Insert &gt; R” at the top of the editor pane type by hand ```{r} ``` if you haven’t deleted a chunk that came with the new file, edit that one Now, let’s write some R code. x &lt;- 4*3 x Now, hitting return does not execute this command; remember, it’s just a text file. To execute it, we need to get what we typed in the the R chunk (the grey R code) down into the console. How do we do it? There are several ways (let’s do each of them): copy-paste this line into the console. select the line (or simply put the cursor there), and click ‘Run’. This is available from the bar above the file (green arrow) the menu bar: Code &gt; Run Selected Line(s) keyboard shortcut: command-return click the green arrow at the right of the code chunk 7.4.3 Your turn Add a few more commands to your file. Execute them by trying the three ways above. Then, save your R Markdown file. 7.5 R functions, help pages So far we’ve learned some of the basic syntax and concepts of R programming, how to navigate RStudio, and RMarkdown, but we haven’t done any complicated or interesting programming processes yet. This is where functions come in! A function is a way to group a set of commands together to undertake a task in a reusable way. When a function is executed, it produces a return value. We often say that we are “calling” a function when it is executed. Functions can be user defined and saved to an object using the assignment operator, so you can write whatever functions you need, but R also has a mind-blowing collection of built-in functions ready to use. To start, we will be using some built in R functions. All functions are called using the same syntax: function name with parentheses around what the function needs in order to do what it was built to do. The pieces of information that the function needs to do its job are called arguments. So the syntax will look something like: result_value &lt;- function_name(argument1 = value1, argument2 = value2, ...). 7.5.1 A simple example To take a very simple example, let’s look at the mean() function. As you might expect, this is a function that will take the mean of a set of numbers. Very convenient! Let’s create our vector of weights again: weight_kg &lt;- c(55, 25, 12) and use the mean function to calculate the mean weight. mean(weight_kg) ## [1] 30.66667 7.5.2 Getting help What if you know the name of the function that you want to use, but don’t know exactly how to use it? Thankfully RStudio provides an easy way to access the help documentation for functions. To access the help page for mean, enter the following into your console: ?mean The help pane will show up in the lower right hand corner of your RStudio. The help page is broken down into sections: Description: An extended description of what the function does. Usage: The arguments of the function(s) and their default values. Arguments: An explanation of the data each argument is expecting. Details: Any important details to be aware of. Value: The data the function returns. See Also: Any related functions you might find useful. Examples: Some examples for how to use the function. 7.5.3 Your turn Exercise: Talk to your neighbor(s) and look up the help file for a function that you know or expect to exist. Here are some ideas: ?getwd(), ?plot(), min(), max(), ?log()). And there’s also help for when you only sort of remember the function name: double-questionmark: ??install Not all functions have (or require) arguments: date() ## [1] &quot;Mon Oct 21 09:57:15 2019&quot; 7.5.4 Use a function to read a file into R So far we have learned how to assign values to objects in R, and what a function is, but we haven’t quite put it all together yet with real data yet. To do this, we will introduce the function read.csv, which will be in the first lines of many of your future scripts. It does exactly what it says, it reads in a csv file to R. Since this is our irst time using this function, first access the help page for read.csv. This has a lot of information in it, as this function has a lot of arguments, and the first one is especially important - we have to tell it what file to look for. Let’s get a file! 7.5.4.1 Download a file from the Arctic Data Center Navigate to this dataset by Craig Tweedie that is published on the Arctic Data Center. Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X., and download the first csv file called “BGchem2008data.csv” Move this file from your Downloads folder into a place you can more easily find it. I recommend creating a folder called data in your previously-created directory arctic_training_files, and putting the file there. Now we have to tell read.csv how to find the file. We do this using the file argument which you can see in usage section in the help page. In RMarkdown, you can either use absolute paths (which will start with your home directory ~/) or paths relative to the location of the RMarkdown. RStudio and RMarkdown have some great autocomplete capabilities when using relative paths, so we will go that route. Assuming you have moved your file to a folder within arctic_training_files called data, your read.csv call will look like this: bg_chem &lt;- read.csv(&quot;data/BGchem2008data.csv&quot;) You should now have an object of the class data.frame in your environment called bg_chem. Check your environment pane to ensure this is true. Note that in the help page there are a whole bunch of arguments that we didn’t use in the call above. Some of the arguments in function calls are optional, and some are required. Optional arguments will be shown in the usage section with a name = value pair, with the default value shown. If you do not specify a name = value pair for that argument in your function call, the function will assume the default value (example: header = TRUE for read.csv). Required arguments will only show the name of the argument, without a value. Note that the only required argument for read.csv is file. You can always specify arguments in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want file = &quot;data/BGchem2008data.csv&quot;, since file is the first argument. If we wanted to add another argument, say stringsAsFactors, we need to specify it explicitly using the name = value pair, since the second argument is header. For functions I call often, I use this resolve by position for the first argument or maybe the first two. After that, I always use name = value. Many R users (including myself) will override the default stringsAsFactors argument using the following call: bg_chem &lt;- read.csv(&quot;data/BGchem2008data.csv&quot;, stringsAsFactors = FALSE) 7.6 Using data.frames A data.frame is a two dimensional data structure in R that mimics spreadsheet behavior. It is a collection of rows and columns of data, where each column has a name and represents a variable, and each row represents a measurement of that variable. When we ran read.csv, the object bg_chem that we created is a data.frame. There are a a bunch of ways R and RStudio help you explore data frames. Here are a few, give them each a try: click on the word bg_chem in the environment pane click on the arrow next to bg_chem in the environment pane execute head(bg_chem) in the console execute View(bg_chem) in the console Usually we will want to run functions on individual columns in a data.frame. To call a specific column, we use the list subset operator $. Say you want to look at the first few rows of the Date column only. This would do the trick: head(bg_chem$Date) ## [1] &quot;2008-03-21&quot; &quot;2008-03-21&quot; &quot;2008-03-21&quot; &quot;2008-03-21&quot; &quot;2008-03-21&quot; ## [6] &quot;2008-03-22&quot; How about calculating the mean temperature of all the CTD samples? mean(bg_chem$CTD_Temperature) ## [1] -0.9646915 Or, if we want to save this to a variable to use later: mean_temp &lt;- mean(bg_chem$CTD_Temperature) You can also create basic plots using the list subset operator. plot(bg_chem$CTD_Depth, bg_chem$CTD_Temperature) There are many more advancted tools and functions in R that will enable you to make better plots using cleaner syntax, we will cover some of these later in the course. 7.6.1 Your Turn Exercise: Spend a few minutes exploring this dataset. Try out different functions on columns using the list subset operator and experiment with different plots. 7.7 Troubleshooting 7.7.1 My RMarkdown won’t knit to PDF If you get an error when trying to knit to PDF that says your computer doesn’t have a LaTeX installation, one of two things is likely happening: Your computer doesn’t have LaTeX installed You have an installation of LaTeX but RStudio cannot find it (it is not on the path) If you already use LaTeX (like to write papers), you fall in the second category. Solving this requires directing RStudio to your installation - and isn’t covered here. If you fall in the first category - you are sure you don’t have LaTeX installed - can use the R package tinytex to easily get an installation recognized by RStudio, as long as you have administrative rights to your computer. To install tinytex run: install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() If you get an error that looks like destination /usr/local/bin not writable, you need to give yourself permission to write to this directory (again, only possible if you have administrative rights). To do this, run this command in the terminal: sudo chown -R `whoami`:admin /usr/local/bin and then try the above install instructions again. More information about tinytex can be found here 7.7.2 I just entered a command and nothing is happening It may be because you didn’t complete a command: is there a little + in your console? R is saying that it is waiting for you to finish. In the example below, I need to close that parenthesis. &gt; x &lt;- seq(1, 10 + You can either just type the closing parentheses here and push return, or push the esc button twice. 7.7.3 R says my object is not found New users will frequently see errors that look like this: Error in mean(myobject) : object 'myobject' not found This means that you do not have an object called myobject saved in your environment. The common reasons for this are: typo: make sure your object name is spelled exactly like what shows up in the console. Remember R is case sensitive. not writing to a variable: note that the object is only saved in the environment if you use the assignment operator, eg: myobject &lt;- read.csv(...) not executing the line in your RMarkdown: remember that writing a line of code in RMarkdown is not the same as writing in the console, you have to execute the line of code using command + enter, running the chunk, or one of the other ways outlined in the RMarkdown section of this training 7.8 Literate Analysis RMarkdown is an excellent way to generate literate analysis, and a reproducible workflow. Here is an example of a real analysis workflow written using RMarkdown. "],
["version-control-with-git-and-github.html", "8 Version Control With git and GitHub 8.1 Learning Objectives 8.2 The problem with filenames 8.3 Version control and Collaboration using Git and GitHub 8.4 Let’s look at a GitHub repository 8.5 The Git lifecycle 8.6 Create a remote repository on GitHub 8.7 Working locally with Git via RStudio 8.8 On good commit messages 8.9 Collaboration and conflict free workflows 8.10 Exercise 8.11 Advanced topics", " 8 Version Control With git and GitHub 8.1 Learning Objectives In this lesson, you will learn: Why git is useful for reproducible analysis How to use git to track changes to your work over time How to use GitHub to collaborate with others How to structure your commits so your changes are clear to others How to write effective commit messages 8.2 The problem with filenames Every file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when problems are discovered. Data files get combined together, then errors are fixed, and then they are split and combined again. In the course of a single analysis, one can expect thousands of changes to files. And yet, all we use to track this are simplistic filenames. You might think there is a better way, and you’d be right: version control. Version control systems help you track all of the changes to your files, without the spaghetti mess that ensues from simple file renaming. In version control systems like git, the system tracks not just the name of the file, but also its contents, so that when contents change, it can tell you which pieces went where. It tracks which version of a file a new version came from. So its easy to draw a graph showing all of the versions of a file, like this one: Version control systems assign an identifier to every version of every file, and track their relationships. They also allow branches in those versions, and merging those branches back into the main line of work. They also support having multiple copies on multiple computers for backup, and for collaboration. And finally, they let you tag particular versions, such that it is easy to return to a set of files exactly as they were when you tagged them. For example, the exact versions of data, code, and narrative that were used when a manuscript was submitted might be R2 in the graph above. 8.3 Version control and Collaboration using Git and GitHub Let’s distinguish between git and GitHub: git: version control software used to track files in a folder (a repository) git creates the versioned history of a repository GitHub: web site that allows users to store their git repositories and share them with others 8.4 Let’s look at a GitHub repository This screen shows the copy of a repository stored on GitHub, with its list of files, when the files and directories were last modified, and some information on who made the most recent changes. If we drill into the “commits” for the repository, we can see the history of changes made to all of the files. Looks like kellijohnson and seananderson were fixing things in June and July: And finally, if we drill into the changes made on June 13, we can see exactly what was changed in each file: Tracking these changes, how they relate to released versions of software and files is exactly what Git and GitHub are good for. And we will show how they can really be effective for tracking versions of scientific code, figures, and manuscripts to accomplish a reproducible workflow. 8.5 The Git lifecycle As a git user, you’ll need to understand the basic concepts associated with versioned sets of changes, and how they are stored and moved across repositories. Any given git repository can be cloned so that it exist both locally, and remotely. But each of these cloned repositories is simply a copy of all of the files and change history for those files, stored in git’s particular format. For our purposes, we can consider a git repository just a folder with a bunch of additional version-related metadata. In a local git-enabled folder, the folder contains a workspace containing the current version of all files in the repository. These working files are linked to a hidden folder containing the ‘Local repository’, which contains all of the other changes made to the files, along with the version metadata. So, when working with files using git, you can use git commands to indicate specifically which changes to the local working files should be staged for versioning (using the git add command), and when to record those changes as a version in the local repository (using the command git commit). The remaining concepts are involved in synchronizing the changes in your local repository with changes in a remote repository. The git push command is used to send local changes up to a remote repository (possibly on GitHub), and the git pull command is used to fetch changes from a remote repository and merge them into the local repository. git clone: to copy a whole remote repository to local git add (stage): notify git to track particular changes git commit: store those changes as a version git pull: merge changes from a remote repository to our local repository git push: copy changes from our local repository to a remote repository git status: determine the state of all files in the local repository git log: print the history of changes in a repository Those seven commands are the majority of what you need to successfully use git. But this is all super abstract, so let’s explore with some real examples. 8.6 Create a remote repository on GitHub Let’s start by creating a repository on GitHub, then we’ll edit some files. Log into GitHub Click the New repository button Name it sasap-test Create a README.md Set the LICENSE to Apache 2.0 You’ve now created your first repository! It has a couple of files that GitHub created for you, like the README.md file, and the LICENSE file, and the .gitignore file. For simple changes to text files, you can make edits right in the GitHub web interface. For example, navigate to the README.md file in the file listing, and edit it by clicking on the pencil icon. This is a regular Markdown file, so you can just add text, and when done, add a commit message, and hit the Commit changes button. Congratulations, you’ve now authored your first versioned commit. If you navigate back to the GitHub page for the repository, you’ll see your commit listed there, as well as the rendered README.md file. Let’s point out a few things about this window. It represents a view of the repository that you created, showing all of the files in the repository so far. For each file, it shows when the file was last modified, and the commit message that was used to last change each file. This is why it is important to write good, descriptive commit messages. In addition, the blue header above the file listing shows the most recent commit, along with its commit message, and its SHA identifer. That SHA identifier is the key to this set of versioned changes. If you click on the SHA identifier (810f314), it will display the set of changes made in that particular commit. In the next section we’ll use the GitHub URL for the GitHub repository you created to clone the repository onto your local machine so that you can edit the files in RStudio. To do so, start by copying the GitHub URL, which represents the repository location: 8.7 Working locally with Git via RStudio RStudio knows how to work with files under version control with Git, but only if you are working within an RStudio project folder. In this next section, we will clone the repository that you created on GitHub into a local repository as an RStudio project. Here’s what we’re going to do: Create the new project Inspect the Git tab and version history Commit a change to the README.md file Commit the changes that RStudio made Inspect the version history Add and commit an Rmd file Push these changes to GitHub View the change history on GitHub Create a New Project. Start by creating a New Project… in R Studio, select the Version Control option, and paste the GitHub URL that you copied into the field for the remote repository Repository URL. While you can name the local copy of the repository anything, its typical to use the same name as the GitHub repository to maintain the correspondence. You can choose any folder for your local copy, in my case I used my standard development folder. Once you hit `Create Project, a new RStudio windo will open with all of the files from the remote repository copied locally. Depending on how your version of RStudio is configured, the location and size of the panes may differ, but they should all be present, including a Git tab and the normal Files tab listing the files that had been created in the remote repository. You’ll note that there is one new file sasap-test.Rproj, and three files that we created earlier on GitHub (.gitignore, LICENSE, and README.md). In the Git tab, you’ll note that two files are listed. This is the status pane that shows the current modification status of all of the files in the repository. In this case, the .gitignore file is listed as M for Modified, and sasap-test.Rproj is listed with a ? ? to indicate that the file is untracked. This means that git has not stored any versions of this file, and knows nothing about the file. As you make version control decisions in RStudio, these icons will change to reflect the current version status of each of the files. Inspect the history. For now, let’s click on the History button in the Git tab, which will show the log of changes that occurred, and will be identical to what we viewed on GitHub. By clicking on each row of the history, you can see exactly what was added and changed in each of the two commits in this repository. Commit a README.md change. Next let’s make a change to the README.md file in RStudio. Add a new section, with a markdown block like this: ## Git from RStudio From within RStudio, we can perform the same versioning actions that we can in GitHub, and much more. Plus, we have the natural advantages of the programming IDE with code completion and other features to make our work easier. - Add files to version control - Commit changes - Push commits to GitHub Once you save, you’ll immediately see the README.md file show up in the Git tab, marks as a modification. You can select the file in the Git tab, and click Diff to see the differences that you saved (but which are not yet committed to your local repository). And here’s what the newly made changes look like compared to the original file. New lines are highlighted in green, while removed lines are in red. Commit the RStudio changes. To commit the changes you made to the README.md file, check the Staged checkbox next to the file (which tells Git which changes you want included in the commit), then provide a descriptive Commit message, and then click Commit. Note that some of the changes in the repository, namely .gitignore and sasap-test.Rproj are still listed as having not been committed. This means there are still pending changes to the repository. You can also see the note that says: Your branch is ahead of ‘origin/master’ by 1 commit. This means that we have committed 1 change in the local repository, but that commit has not yet been pushed up to the origin repository, where origin is the typical name for our remote repository on GitHub. So, let’s commit the remaining project files by staging them and adding a commit message. When finished, you’ll see that no changes remain in the Git tab, and the repository is clean. Inspect the history. Note that the message now says: Your branch is ahead of ‘origin/master’ by 2 commits. These 2 commits are the two we just made, and have not yet been pushed to GitHub. By clicking on the History button, we can see that there are now a total of four commits in the local repository (while there had only been two on GitHub). Push these changes to GitHub. Now that everything has been changed as desired locally, you can push the changes to GitHub using the Push button. This will prompt you for your GitHub username and password, and upload the changes, leaving your repository in a totally clean and synchronized state. When finished, looking at the history shows all four commits, including the two that were done on GitHub and the two that were done locally on RStudio. And note that the labels indicate that both the local repository (HEAD) and the remote repository (origin/HEAD) are pointing at the same version in the history. So, if we go look at the commit history on GitHub, all the commits will be shown there as well. 8.8 On good commit messages Clearly, good documentation of what you’ve done is critical to making the version history of your repository meaningful and helpful. Its tempting to skip the commit message altogether, or to add some stock blurd like ‘Updates’. Its better to use messages that will be helpful to your future self in deducing not just what you did, but why you did it. Also, commit messaged are best understood if they follow the active verb convention. For example, you can see that my commit messages all started with a past tense verb, and then explained what was changed. While some of the changes we illustrated here were simple and so easily explained in a short phrase, for more complext changes, its best to provide a more complete message. The convention, however, is to always have a short, terse first sentence, followed by a more verbose explanation of the details and rationale for the change. This keeps the high level details readable in the version log. I can’t count the number of times I’ve looked at the commit log from 2, 3, or 10 years prior and been so grateful for diligence of my past self and collaborators. 8.9 Collaboration and conflict free workflows Up to now, we have been focused on using Git and GitHub for yourself, which is a great use. But equally powerful is to share a GitHib repository with other researchers so that you can work on code, analyses, and models together. When working together, you will need to pay careful attention to the state of the remote repository to avoid and handle merge conflicts. A merge conflict occurs when two collaborators make two separate commits that change the same lines of the same file. When this happens, git can’t merge the changes together automatically, and will give you back an error asking you to resolve the conflict. Don’t be afraid of merge conflicts, they are pretty easy to handle. and there are some great guides. That said, its truly painless if you can avoid merge conflicts in the first place. You can minimize conflicts by: Ensure that you pull down changes just before you commit Ensures that you have the most recent changes But you may have to fix your code if conflict would have occurred Coordinate with your collaborators on who is touching which files You still need to comunicate to collaborate 8.10 Exercise Use RStudio to add a new RMarkdown file to your sasap-test repository, build a basic structure for the file, and then save it. Next, stage and commit the file locally, and push it up to GitHub. 8.11 Advanced topics There’s a lot we haven’t covered in this brief tutorial. There are some great and much longer tutorials that cover advanced topics, such as: Using git on the command line Resolving conflicts Branching and merging Pull requests versus direct contributions for collaboration Using .gitignore to protect sensitive data GitHub Issues and why they are useful and much, much more Try Git is a great interactive tutorial Software Carpentry Version Control with Git Codecademy Learn Git (some paid) "],
["git-collaboration-and-conflict-management.html", "9 Git: Collaboration and Conflict Management 9.1 Learning Objectives 9.2 Collaborating with Git 9.3 Merge conflicts 9.4 How to resolve a conflict 9.5 Workflows to avoid merge conflicts", " 9 Git: Collaboration and Conflict Management 9.1 Learning Objectives In this lesson, you will learn: How to use Git and GitHub to collaborate with colleagues on code What typically causes conflicts when collaborating Workflows to avoid conflicts How to resolve a conflict 9.2 Collaborating with Git Git is a great tool for working on your own, but even better for working with friends and colleagues. Git allows you to work with confidence on your own local copy of files with the confidence that you will be able to successfully synchronize your changes with the changes made by others. The simplest way to collaborate with Git is to use a shared repository on a hosting service such as GitHub, and use this shared repository as the mechanism to move changes from one collaborator to another. While there are other more advanced ways to sync git repositories, this “hub and spoke” model works really well due to its simplicity. 9.2.1 Activity: Collaborating with a trusted colleague Settings. Working in pairs, choose one person as the ‘Owner’ and one as the ‘Collaborator’. Then, have the Owner visit their arctic-training-repo repository created earlier, and visit the Settings page, and select the Collaborators screen, and add the username of your Collaborator in the box. Once the collaborator has been added, they should check their email for an invitation from GitHub, and click on the acceptance link, which will enable them to collaborate onthe repository. Collaborator clone. To be able to contribute to a repository, the collaborator must clone the repository from the Owner’s github account. To do this, the Collaborator should visit the github page for the Owner’s repository, and then copy the clone URL. In R Studio, the Collaborator will create a new project from version control by pasting this clone URL into the appropriate dialog (see the earlier chapter introducing GitHub). Collaborator Edits. With a clone copied locally, the Collaborator can now make changes to the index.Rmd file in the repository, adding a line or statment somewhere noticeable near the top. Save your changes. Collaborator commit and push. To sync changes, the collaborator will need to add, commit, and push their changes to the Owner’s repository. But before doing so, its good practice to pull immediately before committing to ensure you have the most recent changes from the owner. So, in R Studio’s Git tab, first click the “Diff” button to open the git window, and then press the green “Pull” down arrow button. This will fetch any recent changes from the origin repository and merge them. Next, add the changed index.Rmd file to be committed by cicking the checkbox next to it, type in a commit message, and click ‘Commit.’ Once that finishes, then the collaborator can immediately click ‘Push’ to send the commits to the Owner’s GitHub repository. Owner pull. Now, the owner can open their local working copy of the code in RStudio, and pull those changes down to their local copy. Congrats, the owner now has your changes! Owner edits, commit, and push. Next, the owner should do the same. Make changes to a file in the repository, save it, pull to make sure no new changes have been made while editing, and then add, commit, and push the Owner changes to GitHub. Collaborator pull. The collaborator can now pull down those owner changes, and all copies are once again fully synced. And you’re off to collaborating. 9.3 Merge conflicts A merge conflict occurs when both the owner and collaborator change the same lines in the same file without first pulling the changes that the other has made. This is most easily avoided by good communication about who is working on various sections of each file, and trying to avoid overlaps. But sometimes it happens, and git is there to warn you about potential problems. And git will not allow you to overwrite one person’s changes to a file with another’s changes to the same file if they were based on the same version. The main problem with merge conflicts is that, when the Owner and Collaborator both make changes to the same line of a file, git doesn’t know whose changes take precedence. You have to tell git whose changes to use for that line. 9.4 How to resolve a conflict 9.4.1 Abort, abort, abort… Sometimes you just made a mistake. When you get a merge conflict, the repository is placed in a ‘Merging’ state until you resolve it. There’s a commandline command to abort doing the merge altogether: git merge --abort Of course, after doing that you stull haven’t synced with your collaborator’s changes, so things are still unresolved. But at least your repository is now usable on your local machine. 9.4.2 Checkout The simplest way to resolve a conflict, given that you know whose version of the file you want to keep, is to use the commandline git program to tell git to use either your changes (the person doing the merge), or their changes (the other collaborator). keep your collaborators file: git checkout --theirs conflicted_file.Rmd keep your own file: git checkout --ours conflicted_file.Rmd Once you have run that command, then run add, commit, and push the changes as normal. 9.4.3 Pull and edit the file But that requires the commandline. If you want to resolve from RStudio, or if you want to pick and choose some of your changes and some of your collaborator’s, then instead you can manually edit and fix the file. When you pulled the file with a conflict, git notices that there is a conflict and modifies the file to show both your own changes and your collaborator’s changes in the file. It also shows the file in the Git tab with an orange U icon, which indicates that the file is Unmerged, and therefore awaiting you help to resolve the conflict. It delimits these blocks with a series of less than and greater than signs, so they are easy to find: To resolve the conficts, simply find all of these blocks, and edit them so that the file looks how you want (either pick your lines, your collaborators lines, some combination, or something altogether new), and save. Be sure you removed the delimiter lines that started with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. Once you have made those changes, you simply add, commit, and push the files to resolve the conflict. 9.5 Workflows to avoid merge conflicts Communicate often. Tell each other what you are working on. Pull -&gt; Edit -&gt; Add -&gt; Pull -&gt; Commit -&gt; Push Pull before every commit, and commit often in small chunks. "],
["data-modeling-tidy-data.html", "10 Data Modeling &amp; Tidy Data 10.1 Learning Objectives 10.2 Benefits of relational data systems 10.3 Data Organization 10.4 Multiple tables 10.5 Inconsistent observations 10.6 Inconsistent variables 10.7 Marginal sums and statistics 10.8 Good enough data modeling 10.9 Primary and Foreign Keys 10.10 Entity-Relationship Model (ER) 10.11 Merging data 10.12 Simple Guidelines for Effective Data 10.13 Data modeling exercise 10.14 Related resources", " 10 Data Modeling &amp; Tidy Data 10.1 Learning Objectives Understand basics of relational data models aka tidy data Learn how to design and create effective data tables 10.2 Benefits of relational data systems Powerful search and filtering Handle large, complex data sets Enforce data integrity Decrease errors from redundant updates 10.3 Data Organization 10.4 Multiple tables 10.5 Inconsistent observations 10.6 Inconsistent variables 10.7 Marginal sums and statistics 10.8 Good enough data modeling 10.8.1 Denormalized data Observations about different entities combined In the above example, each row has measurements about both the site at which observations occurred, as well as observations of two individuals of possibly different species found at that site. This is not normalized data. People often refer to this as wide format, because the observations are spread across a wide number of columns. Note that, should one encounter a new species in the survey, we wold have to add new columns to the table. This is difficult to analyze, understand, and maintain. 10.8.2 Tabular data Observations. A better way to model data is to organize the observations about each type of entity in its own table. This results in: Separate tables for each type of entity measured Each row represents a single observed entity Observations (rows) are all unique This is normalized data (aka tidy data) Variables. In addition, for normalized data, we expect the variables to be organized such that: All values in a column are of the same type All columns pertain to the same observed entity (e.g., row) Each column represents either an identifying variable or a measured variable Here’s an example of tidy (normalized) data in which the top table is the collection of observations about individuals of several species, and the bottom table are the observations containing properties of the sites at which the species occurred. 10.9 Primary and Foreign Keys When one has normalized data, we often use unique identifiers to reference particular observations, which allows us to link across tables. Two types of identifiers are common within relational data: Primary Key: unique identifier for each observed entity, one per row Foreign Key: reference to a primary key in another table (linkage) For example, in the second table below, the site column is the primary key of that table, because it uniquely identifies each row of the table as a unique observation of a site. Inthe first table, however, the site column is a foreign key that references the primary key from the second table. This linkage tells us that the first height measurement for the DAPU observation occurred at the site with the name Taku. 10.10 Entity-Relationship Model (ER) An Entity-Relationship model allows us to compactly draw the structure of the tables in a relational database, including the primary and foreign keys in the tables. In the above model, one can see that each site in the SITES table must have one or more observations in the PLOTOBS table, whereas each PLOTOBS has one and only one SITE. 10.11 Merging data Frequently, analysis of data will require merging these separately managed tables back together. There are multiple ways to join the observations in two tables, based on how the rows of one table are merged with the rows of the other. When conceptualizing merges, one can think of two tables, one on the left and one on the right. The most common (and often useful) join is when you merge the subset of rows that have matches in both the left table and the right table: this is called an INNER JOIN. Other types of join are possible as well. A LEFT JOIN takes all of the rows from the left table, and merges on the data from matching rows in the right table. A RIGHT JOIN is the same, except that all of the rows from the right table are included with matching data from the left. Finally, a FULL OUTER JOIN includes all data from all rows in both tables (and is rarely practical). In the figure above, the blue regions show the set of rows that are included in the result. For the INNER join, the rows returned are all rows in A that have a matching row in B. 10.12 Simple Guidelines for Effective Data Design to add rows, not columns Each column one type Eliminate redundancy Uncorrected data file Header line Nonproprietary formats Descriptive names No spaces Borer et al. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America. White et al. 2013. Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution 6. 10.13 Data modeling exercise Break into groups, 1 per table To demonstrate, we’ll be working with a tidied up version of a dataset from ADF&amp;G containing commercial catch data from 1878-1997. The dataset and reference to the original source can be viewed at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2. That site includes metadata describing the full data set, including column definitions. Here’s the first catch table: And here’s the region_defs table: Draw an ER model for the tables Indicate the primary and foreign keys Is the catch table in normal (aka tidy) form? If so, what single type of entity was observed? If not, how might you restructure the data table to make it tidy? Draw a new ER diatram showing this re-designed data structure 10.14 Related resources Borer et al. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America. White et al. 2013. Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution 6. Software Carpentry SQL tutorial Tidy Data "],
["data-cleaning-and-manipulation.html", "11 Data Cleaning and Manipulation 11.1 Learning Objectives 11.2 Introduction 11.3 Setup 11.4 About the pipe (%&gt;%) operator 11.5 Selecting/removing columns: select() 11.6 Quality Check 11.7 Changing column content: mutate() 11.8 Changing shape: pivot_longer() and pivot_wider() 11.9 Renaming columns with rename() 11.10 Adding columns: mutate() 11.11 group_by and summarise 11.12 Filtering rows: filter() 11.13 Sorting your data: arrange() 11.14 Joins in dplyr 11.15 separate() and unite() 11.16 Summary", " 11 Data Cleaning and Manipulation 11.1 Learning Objectives In this lesson, you will learn: What the Split-Apply-Combine strategy is and how it applies to data The difference between wide vs. tall table formats and how to convert between them How to use dplyr and tidyr to clean and manipulate data for analysis How to join multiple data.frames together using dplyr 11.2 Introduction The data we get to work with are rarely, if ever, in the format we need to do our analyses. It’s often the case that one package requires data in one format, while another package requires the data to be in another format. To be efficient analysts, we should have good tools for reformatting data for our needs so we can do our actual work like making plots and fitting models. The dplyr and tidyr R packages provide a fairly complete and extremely powerful set of functions for us to do this reformatting quickly and learning these tools well will greatly increase your efficiency as an analyst. Analyses take many shapes, but they often conform to what is known as the Split-Apply-Combine strategy. This strategy follows a usual set of steps: Split: Split the data into logical groups (e.g., area, stock, year) Apply: Calculate some summary statistic on each group (e.g. mean total length by year) Combine: Combine the groups back together into a single table Figure 1: diagram of the split apply combine strategy As shown above (Figure 1), our original table is split into groups by year, we calculate the mean length for each group, and finally combine the per-year means into a single table. dplyr provides a fast and powerful way to express this. Let’s look at a simple example of how this is done: Assuming our length data is already loaded in a data.frame called length_data: year length_cm 1991 5.673318 1991 3.081224 1991 4.592696 1992 4.381523 1992 5.597777 1992 4.900052 1992 4.139282 1992 5.422823 1992 5.905247 1992 5.098922 We can do this calculation using dplyr like this: length_data %&gt;% group_by(year) %&gt;% summarise(mean_length_cm = mean(length_cm)) Another exceedingly common thing we need to do is “reshape” our data. Let’s look at an example table that is in what we will call “wide” format: site 1990 1991 … 1993 gold 100 118 … 112 lake 100 118 … 112 … … … … … dredge 100 118 … 112 You are probably quite familiar with data in the above format, where values of the variable being observed are spread out across columns (Here: columns for each year). Another way of describing this is that there is more than one measurement per row. This wide format works well for data entry and sometimes works well for analysis but we quickly outgrow it when using R. For example, how would you fit a model with year as a predictor variable? In an ideal world, we’d be able to just run: lm(length ~ year) But this won’t work on our wide data because lm needs length and year to be columns in our table. Or how would we make a separate plot for each year? We could call plot one time for each year but this is tedious if we have many years of data and hard to maintain as we add more years of data to our dataset. The tidyr package allows us to quickly switch between wide format and what is called tall format using the pivot_longer function: site_data %&gt;% pivot_longer(-site, names_to = &quot;year&quot;, values_to = &quot;length&quot;) site year length gold 1990 101 lake 1990 104 dredge 1990 144 … … … dredge 1993 145 In this lesson we’re going to walk through the functions you’ll most commonly use from the dplyr and tidyr packages: dplyr mutate() group_by() summarise() select() filter() arrange() left_join() rename() tidyr pivot_longer() pivot_wider() extract() separate() 11.3 Setup Let’s start going over the most common functions you’ll use from the dplyr package. To demonstrate, we’ll be working with a tidied up version of a dataset from ADF&amp;G containing commercial catch data from 1878-1997. The dataset and reference to the original source can be found at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2. First, let’s load dplyr and tidyr: library(dplyr) library(tidyr) Then let’s read in the data and take a look at it: catch_original &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) head(catch_original) ## Region Year Chinook Sockeye Coho Pink Chum All notesRegCode ## 1 SSE 1886 0 5 0 0 0 5 ## 2 SSE 1887 0 155 0 0 0 155 ## 3 SSE 1888 0 224 16 0 0 240 ## 4 SSE 1889 0 182 11 92 0 285 ## 5 SSE 1890 0 251 42 0 0 292 ## 6 SSE 1891 0 274 24 0 0 298 Note: I copied the URL from the Download button on https://knb.ecoinformatics.org/#view/df35b.304.2 This dataset is relatively clean and easy to interpret as-is. But while it may be clean, it’s in a shape that makes it hard to use for some types of analyses so we’ll want to fix that first. 11.4 About the pipe (%&gt;%) operator Before we jump into learning tidyr and dplyr, we first need to explain the %&gt;%. Both the tidyr and the dplyr packages use the pipe operator - %&gt;%, which may look unfamiliar. The pipe is a powerful way to efficiently chain together operations. The pipe will take the output of a previous statement, and use it as the input to the next statement. Say you want to both filter out rows of a dataset, and select certain columns. Instead of writing df_filtered &lt;- filter(df, ...) df_selected &lt;- select(df_filtered, ...) You can write df_cleaned &lt;- df %&gt;% filter(...) %&gt;% select(...) If you think of the assignment operator (&lt;-) as reading like “gets”, then the pipe operator would read like “then.” So you might think of the above chunk being translated as: The cleaned dataframe gets the original data, and then a filter (of the original data), and then a select (of the filtered data). The benefits to using pipes are that you don’t have to keep track of (or overwrite) intermediate data frames. The drawbacks are that it can be more difficult to explain the reasoning behind each step, especially when many operations are chained together. It is good to strike a balance between writing efficient code (chaining operations), while ensuring that you are still clearly explaining, both to your future self and others, what you are doing and why you are doing it. RStudio has a keyboard shortcut for %&gt;% : Ctrl + Shift + M (Windows), Cmd + Shift + M (Mac). 11.5 Selecting/removing columns: select() The first issue is the extra columns All and notesRegCode. Let’s select only the columns we want, and assign this to a variable called catch_data. catch_data &lt;- catch_original %&gt;% select(Region, Year, Chinook, Sockeye, Coho, Pink, Chum) head(catch_data) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 Much better! select also allows you to say which columns you don’t want, by passing unquoted column names preceded by minus (-) signs: catch_data &lt;- catch_original %&gt;% select(-All, -notesRegCode) head(catch_data) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 11.6 Quality Check Now that we have the data we are interested in using, we should do a little quality check to see that it seems as expected. One nice way of doing this is the summary function. summary(catch_data) ## Region Year Chinook Sockeye ## Length:1708 Min. :1878 Length:1708 Min. : 0.00 ## Class :character 1st Qu.:1922 Class :character 1st Qu.: 6.75 ## Mode :character Median :1947 Mode :character Median : 330.50 ## Mean :1946 Mean : 1401.09 ## 3rd Qu.:1972 3rd Qu.: 995.50 ## Max. :1997 Max. :44269.00 ## Coho Pink Chum ## Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 ## Median : 41.5 Median : 34.5 Median : 63.0 ## Mean : 150.4 Mean : 2357.8 Mean : 422.0 ## 3rd Qu.: 175.0 3rd Qu.: 1622.5 3rd Qu.: 507.5 ## Max. :3220.0 Max. :53676.0 Max. :10459.0 Notice something seems a bit off? The Chinook catch data are character class. Let’s fix it using the function mutate before moving on. 11.7 Changing column content: mutate() We can use the mutate function to change a column, or to create a new column. First Let’s try to just convert the Chinook catch values to numeric type using the as.numeric() function, and overwrite the old Chinook column. catch_clean &lt;- catch_data %&gt;% mutate(Chinook = as.numeric(Chinook)) ## Warning: NAs introduced by coercion head(catch_clean) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 We get a warning “NAs introduced by coercion” which is R telling us that it couldn’t convert every value to an integer and, for those values it couldn’t convert, it put an NA in its place. This is behavior we commonly experience when cleaning datasets and it’s important to have the skills to deal with it when it crops up. To investigate, let’s isolate the issue. We can find out which values are NAs with a combination of is.na() and which(), and save that to a variable called i. i &lt;- which(is.na(catch_clean$Chinook)) i ## [1] 401 It looks like there is only one problem row, lets have a look at it in the original data. catch_data[i,] ## Region Year Chinook Sockeye Coho Pink Chum ## 401 GSE 1955 I 66 0 0 1 Well that’s odd: The value in catch_thousands is I. It turns out that this dataset is from a PDF which was automatically converted into a CSV and this value of I is actually a 1. Let’s fix it by incorporating the ifelse function to our mutate call, which will change the value of the Chinook column to 1 if the value is equal to I, otherwise it will leave the column as the same value. catch_clean &lt;- catch_data %&gt;% mutate(Chinook = ifelse(Chinook == &quot;I&quot;, 1, Chinook)) %&gt;% mutate(Chinook = as.integer(Chinook)) head(catch_clean) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 11.8 Changing shape: pivot_longer() and pivot_wider() The next issue is that the data are in a wide format and, we want the data in a tall format instead. pivot_longer() from the tidyr package helps us do just this conversion: catch_long &lt;- catch_clean %&gt;% pivot_longer(cols = -c(Region, Year), names_to = &quot;species&quot;, values_to = &quot;catch&quot;) head(catch_long) ## # A tibble: 6 x 4 ## Region Year species catch ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 The syntax we used above for pivot_longer() might be a bit confusing so let’s walk though it. The first argument to pivot_longer is the columns over which we are pivoting. You can select these by listing either the names of the columns you do want to pivot, or the names of the columns you are not pivoting over. The names_to argument takes the name of the column that you are creating from the column names you are pivoting over. The values_to argument takes the name of the column that you are creating from the values in the columns you are pivoting over. The opposite of pivot_longer(), pivot_wider(), works in a similar declarative fashion: catch_wide &lt;- catch_long %&gt;% pivot_wider(names_from = species, values_from = catch) head(catch_wide) ## # A tibble: 6 x 7 ## Region Year Chinook Sockeye Coho Pink Chum ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 11.9 Renaming columns with rename() If you scan through the data, you may notice the values in the catch column are very small (these are supposed to be annual catches). If we look at the metadata we can see that the catch column is in thousands of fish so let’s convert it before moving on. Let’s first rename the catch column to be called catch_thousands: catch_long &lt;- catch_long %&gt;% rename(catch_thousands = catch) head(catch_long) ## # A tibble: 6 x 4 ## Region Year species catch_thousands ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 11.10 Adding columns: mutate() Now let’s use mutate again to create a new column called catch with units of fish (instead of thousands of fish). catch_long &lt;- catch_long %&gt;% mutate(catch = catch_thousands * 1000) head(catch_long) Now let’s remove the catch_thousands column for now since we don’t need it. Note that here we have added to the expression we wrote above by adding another function call (mutate) to our expression. This takes advantage of the pipe operator by grouping together a similar set of statements, which all aim to clean up the catch_long data.frame. catch_long &lt;- catch_long %&gt;% mutate(catch = catch_thousands * 1000) %&gt;% select(-catch_thousands) head(catch_long) ## # A tibble: 6 x 4 ## Region Year species catch ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5000 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 We’re now ready to start analyzing the data. 11.11 group_by and summarise As I outlined in the Introduction, dplyr lets us employ the Split-Apply-Combine strategy and this is exemplified through the use of the group_by() and summarise() functions: mean_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarise(catch_mean = mean(catch)) head(mean_region) ## # A tibble: 6 x 2 ## Region catch_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 ALU 40384. ## 2 BER 16373. ## 3 BRB 2709796. ## 4 CHG 315487. ## 5 CKI 683571. ## 6 COP 179223. Exercise: Find another grouping and statistic to calculate for each group. Exercise: Find out if you can group by multiple variables. Another common use of group_by() followed by summarize() is to count the number of rows in each group. We have to use a special function from dplyr, n(). n_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarize(n = n()) head(n_region) ## # A tibble: 6 x 2 ## Region n ## &lt;chr&gt; &lt;int&gt; ## 1 ALU 435 ## 2 BER 510 ## 3 BRB 570 ## 4 CHG 550 ## 5 CKI 525 ## 6 COP 470 11.12 Filtering rows: filter() filter() is the verb we use to filter our data.frame to rows matching some condition. It’s similar to subset() from base R. Let’s go back to our original data.frame and do some filter()ing: SSE_catch &lt;- catch_long %&gt;% filter(Region == &quot;SSE&quot;) head(SSE_catch) ## # A tibble: 6 x 4 ## Region Year species catch ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5000 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 Exercise: Filter to just catches of over one million fish. Exercise: Filter to just SSE Chinook 11.13 Sorting your data: arrange() arrange() is how we sort the rows of a data.frame. In my experience, I use arrange() in two common cases: When I want to calculate a cumulative sum (with cumsum()) so row order matters When I want to display a table (like in an .Rmd document) in sorted order Let’s re-calculate mean catch by region, and then arrange() the output by mean catch: mean_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarise(mean_catch = mean(catch)) %&gt;% arrange(mean_catch) head(mean_region) ## # A tibble: 6 x 2 ## Region mean_catch ## &lt;chr&gt; &lt;dbl&gt; ## 1 BER 16373. ## 2 KTZ 18836. ## 3 ALU 40384. ## 4 NRS 51503. ## 5 KSK 67642. ## 6 YUK 68646. The default sorting order of arrange() is to sort in ascending order. To reverse the sort order, wrap the column name inside the desc() function: mean_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarise(mean_catch = mean(catch)) %&gt;% arrange(desc(mean_catch)) head(mean_region) ## # A tibble: 6 x 2 ## Region mean_catch ## &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 3184661. ## 2 BRB 2709796. ## 3 NSE 1825021. ## 4 KOD 1528350 ## 5 PWS 1419237. ## 6 SOP 1110942. 11.14 Joins in dplyr So now that we’re awesome at manipulating a single data.frame, where do we go from here? Manipulating more than one data.frame. If you’ve ever used a database, you may have heard of or used what’s called a “join”, which allows us to to intelligently merge two tables together into a single table based upon a shared column between the two. We’ve already covered joins in Data Modeling &amp; Tidy Data so let’s see how it’s done with dplyr. The dataset we’re working with, https://knb.ecoinformatics.org/#view/df35b.304.2, contains a second CSV which has the definition of each Region code. This is a really common way of storing auxiliary information about our dataset of interest (catch) but, for analylitcal purposes, we often want them in the same data.frame. Joins let us do that easily. Let’s look at a preview of what our join will do by looking at a simplified version of our data: Visualisation of our left_join First, let’s read in the region definitions data table and select only the columns we want. Note that I have piped my read.csv result into a select call, creating a tidy chunk that reads and selects the data that we need. region_defs &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) %&gt;% select(code, mgmtArea) head(region_defs) ## code mgmtArea ## 1 GSE Unallocated Southeast Alaska ## 2 NSE Northern Southeast Alaska ## 3 SSE Southern Southeast Alaska ## 4 YAK Yakutat ## 5 PWSmgmt Prince William Sound Management Area ## 6 BER Bering River Subarea Copper River Subarea If you examine the region_defs data.frame, you’ll see that the column names don’t exactly match the image above. If the names of the key columns are not the same, you can explicitly specify which are the key columns in the left and right side as shown below: catch_joined &lt;- left_join(catch_long, region_defs, by = c(&quot;Region&quot; = &quot;code&quot;)) head(catch_joined) ## # A tibble: 6 x 5 ## Region Year species catch mgmtArea ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 SSE 1886 Chinook 0 Southern Southeast Alaska ## 2 SSE 1886 Sockeye 5000 Southern Southeast Alaska ## 3 SSE 1886 Coho 0 Southern Southeast Alaska ## 4 SSE 1886 Pink 0 Southern Southeast Alaska ## 5 SSE 1886 Chum 0 Southern Southeast Alaska ## 6 SSE 1887 Chinook 0 Southern Southeast Alaska Notice that I have deviated from our usual pipe syntax (although it does work here!) because I prefer to see the data.frames that I am joining side by side in the syntax. Another way you can do this join is to use rename to change the column name code to Region in the region_defs data.frame, and run the left_join this way: region_defs &lt;- region_defs %&gt;% rename(Region = code, Region_Name = mgmtArea) catch_joined &lt;- left_join(catch_long, region_defs, by = c(&quot;Region&quot;)) head(catch_joined) Now our catches have the auxiliary information from the region definitions file alongside them. Note: dplyr provides a complete set of joins: inner, left, right, full, semi, anti, not just left_join. 11.15 separate() and unite() separate() and its complement, unite() allow us to easily split a single column into numerous (or numerous into a single). This can come in really handle when we have a date column and we want to group by year or month. Let’s make a new data.frame with fake data to illustrate this: dates_df &lt;- data.frame(date = c(&quot;5/24/1930&quot;, &quot;5/25/1930&quot;, &quot;5/26/1930&quot;, &quot;5/27/1930&quot;, &quot;5/28/1930&quot;), stringsAsFactors = FALSE) dates_df %&gt;% separate(date, c(&quot;month&quot;, &quot;day&quot;, &quot;year&quot;), &quot;/&quot;) ## month day year ## 1 5 24 1930 ## 2 5 25 1930 ## 3 5 26 1930 ## 4 5 27 1930 ## 5 5 28 1930 Exercise: Split the city column in the following data.frame into city and state_code columns: cities_df &lt;- data.frame(city = c(&quot;Juneau AK&quot;, &quot;Sitka AK&quot;, &quot;Anchorage AK&quot;), stringsAsFactors = FALSE) # Write your solution here unite() does just the reverse of separate(): dates_df %&gt;% separate(date, c(&quot;month&quot;, &quot;day&quot;, &quot;year&quot;), &quot;/&quot;) %&gt;% unite(date, month, day, year, sep = &quot;/&quot;) ## date ## 1 5/24/1930 ## 2 5/25/1930 ## 3 5/26/1930 ## 4 5/27/1930 ## 5 5/28/1930 Exercise: Use unite() on your solution above to combine the cities_df back to its original form with just one column, city: # Write your solution here 11.16 Summary We just ran through the various things we can do with dplyr and tidyr but if you’re wondering how this might look in a real analysis. Let’s look at that now: catch_original &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) region_defs &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) %&gt;% select(code, mgmtArea) mean_region &lt;- catch_original %&gt;% select(-All, -notesRegCode) %&gt;% mutate(Chinook = ifelse(Chinook == &quot;I&quot;, 1, Chinook)) %&gt;% mutate(Chinook = as.numeric(Chinook)) %&gt;% pivot_longer(-c(Region, Year), names_to = &quot;species&quot;, values_to = &quot;catch&quot;) %&gt;% mutate(catch = catch*1000) %&gt;% group_by(Region) %&gt;% summarize(mean_catch = mean(catch)) %&gt;% left_join(region_defs, by = c(&quot;Region&quot; = &quot;code&quot;)) head(mean_region) ## # A tibble: 6 x 3 ## Region mean_catch mgmtArea ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ALU 40384. Aleutian Islands Subarea ## 2 BER 16373. Bering River Subarea Copper River Subarea ## 3 BRB 2709796. Bristol Bay Management Area ## 4 CHG 315487. Chignik Management Area ## 5 CKI 683571. Cook Inlet Management Area ## 6 COP 179223. Copper River Subarea "],
["writing-good-data-management-plans.html", "12 Writing Good Data Management Plans 12.1 Learning Objectives 12.2 Materials", " 12 Writing Good Data Management Plans 12.1 Learning Objectives In this lesson, you will learn: Why create data management plans What are the major components of data management plans Tools that can help create a data management plan Festures and functionality of the DMPTool 12.2 Materials Download slides: Writing Good Data Management Plans "],
["data-visualization-for-web-based-maps.html", "13 Data visualization for web-based maps 13.1 Learning Objectives 13.2 Introduction 13.3 A Minimal Example 13.4 A Less Minimal Example 13.5 Static maps with geom_sf 13.6 Resources", " 13 Data visualization for web-based maps 13.1 Learning Objectives In this lesson, you will learn: How to use RMarkdown to build a web site A quick overview of producing nice visualizations in R with ggplot How to create interactive maps with leaflet Publishing interactive maps using RMarkdown to make a GitHub web site 13.2 Introduction Sharing your work with others in engaging ways is an important part of the scientific process. So far in this course, we’ve introduced a small set of powerful tools for doing open science: R and its many packages RStudio git GitHub RMarkdown RMarkdown, in particular, is amazingly powerful for creating scientific reports but, so far, we haven’t tapped its full potential for sharing our work with others. In this lesson, we’re going to take an existing GitHub repository and turn it into a beautiful and easy to read web page using the tools listed above. 13.3 A Minimal Example In RStudio, create a new file at the top level of your git repository called index.Rmd. The easiest way to do this is through the RStudio menu. Choose File -&gt; New File -&gt; RMarkdown… This will bring up a dialog box. You should create a “Document” in “HTML” format. These are the default options. Open index.Rmd (if it isn’t already open) Press Knit Observe the rendered output Notice the new file in the same directory index.html. This is our RMarkdown file rendered as HTML (a web page) Commit your changes (to both index.Rmd and index.html) Open your web browser to the GitHub.com page for your repository Go to Settings &gt; GitHub Pages and turn on GitHub Pages for the master branch Now, the rendered website version of your repo will show up at a special URL. GitHub Pages follows a convention like this: https://{username}.github.io/{repository}/ https://mbjones.github.io/arctic-training-repo/ Note that it will no longer be at github.com but github.io. Go to https://{username}.github.io/{repo_name}/ (Note the trailing /) Observe the awesome rendered output 13.4 A Less Minimal Example Now that we’ve seen how to create a web page from RMarkdown, let’s create a website that uses some of the cool functionality available to us. We’ll use the same git repository and RStudio Project as above, but we’ll be adding some files to the repository and modifying index.Rmd. First, let’s get some data. We’ll re-use the salmon escapement data from the ADF&amp;G OceanAK database: Navigate to Escapement Counts (or visit the KNB and search for ‘oceanak’) and copy the Download URL for the ADFG_firstAttempt_reformatted.csv file Download that file into R using read.csv to make the script portable Calculate annual escapement by species and region using the dplyr package Make a bar plot of the annual escapement by species using the ggplot2 package Display it in an interactive table with the datatable function from the DT package And lastly, let’s make an interactive, Google Maps-like map of the escapement sampling locations. To do this, we’ll use the leaflet package to create an interactive map with markers for all the sampling locations: First, let’s load the packages we’ll need: library(leaflet) library(dplyr) library(tidyr) library(ggplot2) library(DT) library(sf) library(ggmap) # devtools::install_github(&quot;dkahle/ggmap&quot;) 13.4.1 Load salmon escapement data You can load the data table directly from the KNB Data Repository, if it isn’t already present on your local computer. This technique only downloads the file if you need it. data_url &lt;- &quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Af119a05b-bbe7-4aea-93c6-85434dcb1c5e&quot; esc &lt;- tryCatch( read.csv(&quot;data/escapement.csv&quot;, stringsAsFactors = FALSE), error=function(cond) { message(paste(&quot;Escapement file does not seem to exist, so get it from the KNB.&quot;)) esc &lt;- read.csv(url(data_url, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) return(esc) } ) head(esc) ## Location SASAP.Region sampleDate Species DailyCount Method ## 1 Akalura Creek Kodiak 1930-05-24 Sockeye 4 Unknown ## 2 Akalura Creek Kodiak 1930-05-25 Sockeye 10 Unknown ## 3 Akalura Creek Kodiak 1930-05-26 Sockeye 0 Unknown ## 4 Akalura Creek Kodiak 1930-05-27 Sockeye 0 Unknown ## 5 Akalura Creek Kodiak 1930-05-28 Sockeye 0 Unknown ## 6 Akalura Creek Kodiak 1930-05-29 Sockeye 0 Unknown ## Latitude Longitude Source ## 1 57.1641 -154.2287 ADFG ## 2 57.1641 -154.2287 ADFG ## 3 57.1641 -154.2287 ADFG ## 4 57.1641 -154.2287 ADFG ## 5 57.1641 -154.2287 ADFG ## 6 57.1641 -154.2287 ADFG 13.4.2 Static Plots Now that we have the data loaded, let’s calculate annual escapement by species and region: annual_esc &lt;- esc %&gt;% separate(sampleDate, c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;), sep = &quot;-&quot;) %&gt;% mutate(Year = as.numeric(Year)) %&gt;% group_by(Species, SASAP.Region, Year) %&gt;% summarize(escapement = sum(DailyCount)) %&gt;% filter(Species %in% c(&quot;Chinook&quot;, &quot;Sockeye&quot;, &quot;Chum&quot;, &quot;Coho&quot;, &quot;Pink&quot;)) head(annual_esc) ## # A tibble: 6 x 4 ## # Groups: Species, SASAP.Region [1] ## Species SASAP.Region Year escapement ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Chinook Alaska Peninsula and Aleutian Islands 1974 1092 ## 2 Chinook Alaska Peninsula and Aleutian Islands 1975 1917 ## 3 Chinook Alaska Peninsula and Aleutian Islands 1976 3045 ## 4 Chinook Alaska Peninsula and Aleutian Islands 1977 4844 ## 5 Chinook Alaska Peninsula and Aleutian Islands 1978 3901 ## 6 Chinook Alaska Peninsula and Aleutian Islands 1979 10463 That command used a lot of the dplyr commands that we’ve used, and some that are new. The separate function is used to divide the sampleDate column up into Year, Month, and Day columns, and then we use group_by to indicate that we want to calculate our results for the unique combinations of species, region, and year. We next use summarize to calculate an escapement value for each of these groups. Finally, we use a filter and the %in% operator to select only the salmon species. Now, let’s plot our results using ggplot. ggplot uses a mapping aesthetic (set using aes()) and a geometry to create your plot. Additional geometries/aesthetics and theme elements can be added to a ggplot object using +. ggplot(annual_esc, aes(x = Species, y = escapement)) + geom_col() What if we want our bars to be blue instad of gray? You might think we could run this: ggplot(annual_esc, aes(x = Species, y = escapement, fill = &quot;blue&quot;)) + geom_col() Why did that happen? Notice that we tried to set the fill color of the plot inside the mapping aesthetic call. What we have done, behind the scenes, is create a column filled with the word “blue” in our dataframe, and then mapped it to the fill aesthetic, which then chose the default fill color of red. What we really wanted to do was just change the color of the bars. If we want do do that, we can call the color option in the geom_bar function, outside of the mapping aesthetics function call. ggplot(annual_esc, aes(x = Species, y = escapement)) + geom_col(fill = &quot;blue&quot;) What if we did want to map the color of the bars to a variable, such as region. ggplot is really powerful because we can easily get this plot to visualize more aspects of our data. ggplot(annual_esc, aes(x = Species, y = escapement, fill = SASAP.Region)) + geom_col() Here is an example of a different aesthetic and geometry mapping we can create with this data. You can also add a title, adjust labels, and include a built in theme. ggplot(filter(annual_esc, SASAP.Region == &quot;Kodiak&quot;), aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + ylab(&quot;Escapement&quot;) + ggtitle(&quot;Kodiak Salmon Escapement&quot;) + theme_bw() Note how I added a filter call in my ggplot function call to only plot data from Kodiak. You can easily create plots like this for every region using facet_wrap ggplot(annual_esc, aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + facet_wrap(~SASAP.Region, scales = &quot;free_y&quot;) + ylab(&quot;Escapement&quot;) + theme_bw() 13.4.3 Interactive Maps Now let’s convert the escapement data into a table of just the unique locations: locations &lt;- esc %&gt;% distinct(Location, Latitude, Longitude) %&gt;% drop_na() And display it as an interactive table: datatable(locations) Then making a leaflet map is (generally) only a couple of lines of code: leaflet(locations) %&gt;% addTiles() %&gt;% addMarkers(~ Longitude, ~ Latitude, popup = ~ Location) The addTiles() function gets a base layer of tiles from OpenStreetMap which is an open alternative to Google Maps. addMarkers use a bit of an odd syntax in that it looks kind of like ggplot2 code but uses ~ before the column names. This is similar to how the lm function (and others) work but you’ll have to make sure you type the ~ for your map to work. You can also use leaflet to import Web Map Service (WMS) tiles. Here is an example that utilizes the General Bathymetric Map of the Oceans (GEBCO) WMS tiles. In this example, we also demonstrate how to create a more simple circle marker. leaflet(locations) %&gt;% addWMSTiles(&quot;https://www.gebco.net/data_and_products/gebco_web_services/web_map_service/mapserv?&quot;, layers = &#39;GEBCO_LATEST&#39;, attribution = &quot;Imagery reproduced from the GEBCO_2014 Grid, version 20150318, www.gebco.net&quot;) %&gt;% addCircleMarkers(lng = ~Longitude, lat = ~Latitude, popup = ~ Location, radius = 5, # set fill properties fillColor = &quot;salmon&quot;, fillOpacity = 1, # set stroke properties stroke = T, weight = 0.5, color = &quot;white&quot;, opacity = 1) Leaflet has a ton of functionality that can enable you to create some beautiful, functional maps with relative ease. Here is an example of some we created as part of the SASAP project, created using the same tools we showed you here. This map hopefully gives you an idea of how powerful the combination of RMarkdown and GitHub pages can be. 13.5 Static maps with geom_sf We can also create a similar static map using ggplot with the geom_sf geometry. To do so, first we need to transform our locations data frame into an sf object by providing the coordinate reference system (EPSG:4326 is the CRS for geocoordinates in WGS84). We also transform the points to EPSG:3857 which is the CRS used for rendering maps in Google Maps, Stamen, and OpenStreetMap, among others. locations_sf &lt;- locations %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), crs = 4326) locations_sf_3857 &lt;- st_transform(locations_sf, 3857) Next, let’s grab a base map from the Stamen map tile server covering the region of interest. This includes a fix to transform the bounding box (which starts in EPSSG:326) to also be in the EPSG:3857 CRS, which is the projection that the map raster is returned in from Stamen. # Define a function to fix the bbox to be in EPSG:3857 # See https://github.com/dkahle/ggmap/issues/160#issuecomment-397055208 ggmap_bbox_to_3857 &lt;- function(map) { if (!inherits(map, &quot;ggmap&quot;)) stop(&quot;map must be a ggmap object&quot;) # Extract the bounding box (in lat/lon) from the ggmap to a numeric vector, # and set the names to what sf::st_bbox expects: map_bbox &lt;- setNames(unlist(attr(map, &quot;bb&quot;)), c(&quot;ymin&quot;, &quot;xmin&quot;, &quot;ymax&quot;, &quot;xmax&quot;)) # Coonvert the bbox to an sf polygon, transform it to 3857, # and convert back to a bbox (convoluted, but it works) bbox_3857 &lt;- st_bbox(st_transform(st_as_sfc(st_bbox(map_bbox, crs = 4326)), 3857)) # Overwrite the bbox of the ggmap object with the transformed coordinates attr(map, &quot;bb&quot;)$ll.lat &lt;- bbox_3857[&quot;ymin&quot;] attr(map, &quot;bb&quot;)$ll.lon &lt;- bbox_3857[&quot;xmin&quot;] attr(map, &quot;bb&quot;)$ur.lat &lt;- bbox_3857[&quot;ymax&quot;] attr(map, &quot;bb&quot;)$ur.lon &lt;- bbox_3857[&quot;xmax&quot;] map } bbox &lt;- c(-170, 52, -130, 64) # This is roughly southern Alaska ak_map &lt;- get_stamenmap(bbox, zoom = 4) ak_map_3857 &lt;- ggmap_bbox_to_3857(ak_map) Finally, plot both the base raster map with the points overlayed, which is easy now that everything is in the same projection (3857): ggmap(ak_map_3857) + geom_sf(data = locations_sf_3857, inherit.aes = FALSE) 13.6 Resources Lisa Charlotte Rost. (2018) Why not to use two axes, and what to use instead: The case against dual axis charts "],
["social-aspects-of-collaboration-and-data-policies.html", "14 Social aspects of collaboration and data policies 14.1 Learning Objectives 14.2 Resources 14.3 References", " 14 Social aspects of collaboration and data policies 14.1 Learning Objectives In this lesson, you will learn: Technique for group facilitation Policies surrounding data, authorship and community participation 14.2 Resources Meeting facilitation technique Example Codes of Conduct Carpentries Code of Conduct Mozilla Science Code of Conduct Mozilla Community Participation Guidelines Ecological Society of America Code of Conduct American Geophysical Union Code of Conduct Policy Templates: Authorship Policy Data Policy 14.3 References Cheruvelil, K. S., Soranno, P. A., Weathers, K. C., Hanson, P. C., Goring, S. J., Filstrup, C. T., &amp; Read, E. K. (2014). Creating and maintaining high-performing collaborative research teams: The importance of diversity and interpersonal skills. Frontiers in Ecology and the Environment, 12(1), 31-38. DOI: 10.1890/130001 "],
["creating-r-functions.html", "15 Creating R Functions 15.1 Leaning outcomes 15.2 Why functions? 15.3 Temperature conversion 15.4 Creating a function 15.5 Exercise 15.6 Documenting R functions 15.7 Summary", " 15 Creating R Functions Many people write R code as a single, continuous stream of commands, often drawn from the R Console itself and simply pasted into a script. While any script brings benefits over non-scripted solutions, there are advantages to breaking code into small, reusable modules. This is the role of a function in R. In this lesson, we will review the advantages of coding with functions, practice by creating some functions and show how to call them, and then do some exercises to build other simple functions. 15.1 Leaning outcomes Learn why we should write code in small functions Write code for one or more functions Document functions to improve understanding and code communication 15.2 Why functions? In a word: DRY: Don’t Repeat Yourself By creating small functions that only one logical task and do it well, we quickly gain: Improved understanding Reuse via decomposing tasks into bite-sized chunks Improved error testing 15.3 Temperature conversion Imagine you have a bunch of data measured in Fahrenheit and you want to convert that for analytical purposes to Celsius. You might have an R script that does this for you. airtemps &lt;- c(212, 30.3, 78, 32) celsius1 &lt;- (airtemps[1]-32)*5/9 celsius2 &lt;- (airtemps[2]-32)*5/9 celsius3 &lt;- (airtemps[3]-32)*5/9 Note the duplicated code, where the same formula is repeated three times. This code would be both more compact and more reliable if we didn’t repeat ourselves. 15.4 Creating a function Functions in R are a mechanism to process some input and return a value. Similarly to other variables, functions can be assigned to a variable so that they can be used throughout code by reference. To create a function in R, you use the function function (so meta!) and assign its result to a variable. Let’s create a function that calculates celsius temperature outputs from fahrenheit temperature inputs. fahr_to_celsius &lt;- function(fahr) { celsius &lt;- (fahr-32)*5/9 return(celsius) } By running this code, we have created a function and stored it in R’s global environment. The fahr argument to the function function indicates that the function we are creating takes a single parameter (the temperature in fahrenheit), and the return statement indicates that the function should return the value in the celsius variable that was calculated inside the function. Let’s use it, and check if we got the same value as before: celsius4 &lt;- fahr_to_celsius(airtemps[1]) celsius4 ## [1] 100 celsius1 == celsius4 ## [1] TRUE Excellent. So now we have a conversion function we can use. Note that, because most operations in R can take multiple types as inputs, we can also pass the original vector of airtemps, and calculate all of the results at once: celsius &lt;- fahr_to_celsius(airtemps) celsius ## [1] 100.0000000 -0.9444444 25.5555556 0.0000000 This takes a vector of temperatures in fahrenheit, and returns a vector of temperatures in celsius. 15.5 Exercise Now, create a function named celsius_to_fahr that does the reverse, it takes temperature data in celsius as input, and returns the data converted to fahrenheit. Then use that formula to convert the celsius vector back into a vector of fahrenheit values, and compare it to the original airtemps vector to ensure that your answers are correct. # Your code goes here Did you encounter any issues with rounding or precision? 15.6 Documenting R functions Functions need documentation so that we can communicate what they do, and why. The roxygen2 package provides a simple means to document your functions so that you can explain what the function does, the assumptions about the input values, a description of the value that is returned, and the rationale for decisions made about implementation. Documentation in ROxygen is placed immediately before the function definition, and is indicated by a special comment line that always starts with the characters #'. Here’s a documented version of a function: #&#39; Convert temperature data from Fahrenheit to Celsius #&#39; #&#39; @param fahr Temperature data in degrees Fahrenheit to be converted #&#39; @return temperature value in degrees Celsius #&#39; @keywords conversion #&#39; @export #&#39; @examples #&#39; fahr_to_celsius(32) #&#39; fahr_to_celsius(c(32, 212, 72)) fahr_to_celsius &lt;- function(fahr) { celsius &lt;- (fahr-32)*5/9 return(celsius) } Note the use of the @param keyword to define the expectations of input data, and the @return keyword for defining the value that is returned from the function. The @examples function is useful as a reminder as to how to use the function. Finally, the @export keyword indicates that, if this function were added to a package, then the function should be available to other code and packages to utilize. 15.7 Summary Functions are useful to reduce redundancy, reuse code, and reduce errors Build functions with the function function Document functions with roxygen2 comments "],
["creating-r-packages.html", "16 Creating R Packages 16.1 Learning Objectives 16.2 Why packages? 16.3 Install and load packages 16.4 Create a basic package 16.5 Add your code 16.6 Add documentation 16.7 Test your package 16.8 Checking and installing your package 16.9 Sharing and releasing your package 16.10 More reading", " 16 Creating R Packages 16.1 Learning Objectives In this lesson, you will learn: The advantages of using R packages for organizing code Simple techniques for creating R packages Approaches to documenting code in packages 16.2 Why packages? Most R users are familiar with loading and utilizing packages in their work. And they know how rich CRAN is in providing for many conceivable needs. Most people have never created a package for their own work, and most think the process is too complicated. Really it’s pretty straighforward and super useful in your personal work. Creating packages serves two main use cases: Mechanism to redistribute reusable code (even if just for yourself) Mechanism to reproducibly document analysis and models and their results At a minimum, you can easily produce a package for just your own useful code functions, which makes it easy to maintain and use utilities that span your own projects. The usethis, devtools and roxygen2 packages make creating and maintining a package to be a straightforward experience. 16.3 Install and load packages library(devtools) library(usethis) # install.packages(&quot;roxygen2&quot;) library(roxygen2) 16.4 Create a basic package Thanks to the great usethis package, it only takes one function call to create the skeleton of an R package using create_package(). Which eliminates pretty much all reasons for procrastination. To create a package called mytools, all you do is: setwd(&#39;..&#39;) create_package(&quot;mytools&quot;) ✔ Setting active project to &#39;/Users/jones/development/mytools&#39; ✔ Creating &#39;R/&#39; ✔ Creating &#39;man/&#39; ✔ Writing &#39;DESCRIPTION&#39; ✔ Writing &#39;NAMESPACE&#39; ✔ Writing &#39;mytools.Rproj&#39; ✔ Adding &#39;.Rproj.user&#39; to &#39;.gitignore&#39; ✔ Adding &#39;^mytools\\\\.Rproj$&#39;, &#39;^\\\\.Rproj\\\\.user$&#39; to &#39;.Rbuildignore&#39; ✔ Opening new project &#39;mytools&#39; in RStudio This will create a top-level directory structure, including a number of critical files under the standard R package structure. The most important of which is the DESCRIPTION file, which provides metadata about your package. Edit the DESCRIPTION file to provide reasonable values for each of the fields, including your own contact information. Information about choosing a LICENSE is provided in the Extending R documentation. The DESCRIPTION file expects the license to be chose from a predefined list, but you can use it’s various utility methods for setting a specific license file, such as the Apacxhe 2 license: use_apl2_license(name=&quot;Matthew Jones&quot;) ✔ Setting License field in DESCRIPTION to &#39;Apache License (&gt;= 2.0)&#39; ✔ Writing &#39;LICENSE.md&#39; ✔ Adding &#39;^LICENSE\\\\.md$&#39; to &#39;.Rbuildignore&#39; Once your license has been chosen, and you’ve edited your DESCRIPTION file with your contact information, a title, and a description, it will look like this: Package: mytools Title: Utility functions created by Matt Jones Version: 0.1 Authors@R: &quot;Matthew Jones &lt;jones@nceas.ucsb.edu&gt; [aut, cre]&quot; Description: Package mytools contains a suite of utility functions useful whenever I need stuff to get done. Depends: R (&gt;= 3.5.0) License: Apache License (&gt;= 2.0) LazyData: true 16.5 Add your code The skeleton package created contains a directory R which should contain your source files. Add your functions and classes in files to this directory, attempting to choose names that don’t conflict with existing packages. For example, you might add a file environemnt_info.R that contains a function environment_info() that you might want to reuse. This one might leave something to be desired…, but you get the point… The usethis::use_r() function will help set up you files in the right places. For example, running: use_r(&quot;environment_info&quot;) ● Modify &#39;R/environment_info.R&#39; creates the file R/environment_info.R, which you can then modify to add the implementation fo the following function: environment_info &lt;- function(msg) { print(devtools::session_info()) print(paste(&quot;Also print the incoming message: &quot;, msg)) } If your R code depends on functions from another package, then you must declare so in the Imports list in the DESCRIPTION file for your package. In our example above, we depend on the devtools package, and so we need to list it as a dependency. Once again, usethis provides a handy helper method: usethis::use_package(&quot;devtools&quot;) ✔ Adding &#39;devtools&#39; to Imports field in DESCRIPTION ● Refer to functions with `devtools::fun()` 16.6 Add documentation You should provide documentation for each of your functions and classes. This is done in the roxygen2 approach of providing embedded comments in the source code files, which are in turn converted into manual pages and other R documentation artifacts. Be sure to define the overall purpose of the function, and each of its parameters. #&#39; A function to print information about the current environment. #&#39; #&#39; This function prints current environment information, and a message. #&#39; @param msg The message that should be printed #&#39; @keywords debugging #&#39; @import devtools #&#39; @export #&#39; @examples #&#39; environment_info(&quot;This is an important message from your sponsor.&quot;) environment_info &lt;- function(msg) { print(devtools::session_info()) print(paste(&quot;Also print the incoming message: &quot;, msg)) } Once your files are documented, you can then process the documentation using the document() function to generate the appropriate .Rd files that your package needs. document() Updating mytools documentation Updating roxygen version in /Users/jones/development/mytools/DESCRIPTION Writing NAMESPACE Loading mytools Writing NAMESPACE Writing environment_info.Rd That’s really it. You now have a package that you can check() and install() and release(). See below for these helper utilities. 16.7 Test your package You can tests your code using the tetsthat testing framework. The ussethis::use_testthat() function will set up your package for testing, and then you can use the use_test() function to setup individual test files. For example, if you want to create tests of our environment_info functions, set it up like this: usethis::use_testthat() ✔ Adding &#39;testthat&#39; to Suggests field in DESCRIPTION ✔ Creating &#39;tests/testthat/&#39; ✔ Writing &#39;tests/testthat.R&#39; usethis::use_test(&quot;environment_info&quot;) ✔ Writing &#39;tests/testthat/test-environment_info.R&#39; ● Modify &#39;tests/testthat/test-environment_info.R&#39; You can now add tests to the test-environment_info.R, and you can run all of the tests using devtools::test(). For example, if you add a test to the test-environment_info.R file: test_that(&quot;A message is present&quot;, { capture.output(result &lt;- environment_info(&quot;A unique message&quot;)) expect_match(result, &quot;A unique message&quot;) }) Then you can run the tests to be sure all of your functions are working using devtools::test(): devtools::test() Loading mytools Testing mytools ✔ | OK F W S | Context ✔ | 2 | test-environment_info [0.1 s] ══ Results ════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════ Duration: 0.1 s OK: 2 Failed: 0 Warnings: 0 Skipped: 0 Yay, all tests passed! 16.8 Checking and installing your package Now that your package is built, you can check it for consistency and completeness using check(), and then you can install it locally using install(), which needs to be run from the parent directory of your module. check() install() Your package is now available for use in your local environment. 16.9 Sharing and releasing your package The simplest way to share your package with others is to upload it to a GitHub repository, which allows others to install your package using the install_github('mytools','github_username') function from devtools. If your package might be broadly useful, also consider releasing it to CRAN, using the release() method from `devtools(). Releasing a package to CRAN requires a significant amoutn of work to ensure it follows the standards set by the R community, but it is entirely tractable and a valuable contribution to the science community. If you are considering releasing a package more broadly, you may find that the supportive community at ROpenSci provides incredible help and valuable feeback through their onboarding process. 16.10 More reading Hadley Wickham’s awesome book: R Packages Thomas Westlake’s blog Writing an R package from scratch "],
["programming-metadata-and-data-publishing.html", "17 Programming Metadata and Data Publishing 17.1 Learning Objectives 17.2 Creating Metadata 17.3 Publish data to the Arctic Data Center test site", " 17 Programming Metadata and Data Publishing 17.1 Learning Objectives In this lesson, you will learn: How to write standardized metadata in R How to publish data packages to the Arctic Data Center programmatically For datasets with a relatively small number of files that do not have many columns, the Arctic Data Center web form is the most efficient way to create metadata. However, for datasets that have many hundreds of files with a similar structure, or a large number of attributes, programmatic metadata creation may be more efficent. Creating metadata and publishing programmatically also allows for more a streamlined approach to updating datasets that have been published. By incorporating this documentation and publishing approach into your scientific workflow, you can improve the reproducibility and transparency of your work. 17.2 Creating Metadata 17.2.1 About Ecological Metadata Language (EML) EML, the metadata standard that the Arctic Data Center uses, looks like this: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; Earlier in this course we learned how to create an EML document like this using the Arctic Data Center web form. Now, we will learn how to create it using R. This can be especially useful to decrease the repetitiveness of metadata creation when you have many files with the same format, files with many attributes, or many data packages with a similar format. When you create metadata using the web form, the form creates valid metadata for you. Valid, structured metadata is what enables computers to predictably parse the information in a metadata document, enabling search, display, and even meta-analysis. When we create metadata in R, there aren’t as many user-friendly checks in place to ensure we create valid EML, so we need to understand the structure of the document more completely in order to make sure that it will be compatible with the Arctic Data Center. Let’s look at a simplified version of the example above: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; EML is written in XML (eXtensisble Markup Language). One of the key concepts in XML is the element. An XML element is everything that is encompassed by a start tag (&lt;...&gt;) and an end tag (&lt;/...&gt;). So, a simple element in the example above is &lt;surName&gt;Willette&lt;/surName&gt;. The name of this element is surName and the value is simple text, “Willette”. Each element in EML has a specific (case sensitive!) name and description that is specified in the schema. The description of the surName element is: “The surname field is used for the last name of the individual associated with the resource. This is typically the family name of an individual…” The EML schema specifies not only the names and descriptions of all EML elements, but also certain requirements, like which elements must be included, what type of values are allowed within elements, and how the elements are organized. An EML document is valid when it adheres to all of the requirements speficied in the EML schema. You’ll notice that some elements, rather than having a simple value, instead contain other elements that are nested within them. Let’s look at individualName. &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; This element is a collection of other elements (sometimes referred to as child elements). In this example, the individualName element has a givenName and a surName child element. We can check the requirements of any particular element by looking at the schema documents, which includes some helpful diagrams. The diagram in the individualName section of the schema looks like this: This shows that within individualName there are 3 possible child elements: salutation, givenName, and surName. The yellow circle icon with stacked papers tells you that the elements come in series, so you can include one or more of the child elements (as opposed to a switch symbol, which means that you choose one element from a list of options). The bold line tells you that surName is required, and the 0..inf indicates that you can include 0 or more salultation or givenName elements. So, to summarize, EML is the metadata standard used by the Arctic Data Center. It is written in XML, which consists of a series of nested elements. The element definitions, required elements, and structure are all defined by a schema. When you write EML, in order for it to be valid, your EML document must conform to the requirements given in the schema. 17.2.2 Metadata in R: a simple example Now, let’s learn how the EML package can help us create EML in R. First, load the EML package in R. library(EML) The EML package relies on named list structures to generate name-value pairs for elements. “Named list structures” may sound unfamiliar, but they aren’t dissimilar from data.frames. A data.frame is just a named list of vectors of the same length, where the name of the vector is the column name. In this section, we will use the familiar $ operator to dig down into the named list structure that we generate. To show how this works, let’s create the individualName element, and save it as an object called me. Remember the schema requirements - indvidualName has child elements salutation, givenName, and surName. At least surName is required. me &lt;- list(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;) me ## $givenName ## [1] &quot;Jeanette&quot; ## ## $surName ## [1] &quot;Clark&quot; So we have created the contents of an individualName element, with child elements givenName and surName, and assigned the values of those child elements to my name. This might look confusing, hard to remember, and if you aren’t intimitely familiar with the EML schema, you are probably feeling a little intimidated. Luckily the EML package has a set of helper list constructor functions which tell you what child elements can be used in a parent element. The helper functions have the format eml$elementName(). When combined with the RStudio autocomplete functionality, the whole process gets a lot easier! me &lt;- eml$individualName(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;) We can then use this object me, which represents the individualName element, and insert it into a larger EML document. At the top level of a complete EML document are the packageId and system elements. This is how your document can be uniquely identified within whatever system manages it. The packageId element typically contains the DOI (Digital Object Identifier) or other identifier for the dataset. Nested within the top level is the dataset element. All EML documents must have, at a minimum, a title, creator, and contact, in addition to the packageId and system. Let’s create a minimal valid EML dataset, with an arbitrary packageId and system. doc &lt;- list(packageId = &quot;dataset-1&quot;, system = &quot;local&quot;, dataset = eml$dataset(title = &quot;A minimial valid EML dataset&quot;, creator = eml$creator(individualName = me), contact = eml$contact(individualName = me))) Unlike the web editor, in R there is nothing stopping you from inserting arbitrary elements into your EML document. A critical step to creating EML is validating your EML document to make sure it conforms to the schema requirements. In R this can be done using the eml_validate function. eml_validate(doc) ## [1] TRUE ## attr(,&quot;errors&quot;) ## character(0) We can write our EML using write_eml. write_eml(doc, &quot;files/simple_example.xml&quot;) Here is what the written file looks like: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:stmml=&quot;http://www.xml-cml.org/schema/stmml-1.1&quot; packageId=&quot;id&quot; system=&quot;system&quot; xsi:schemaLocation=&quot;eml://ecoinformatics.org/eml-2.1.1/ eml.xsd&quot;&gt; &lt;dataset&gt; &lt;title&gt;A minimial valid EML dataset&lt;/title&gt; &lt;creator&gt; &lt;individualName&gt; &lt;givenName&gt;Jeanette&lt;/givenName&gt; &lt;surName&gt;Clark&lt;/surName&gt; &lt;/individualName&gt; &lt;/creator&gt; &lt;contact&gt; &lt;individualName&gt; &lt;givenName&gt;Jeanette&lt;/givenName&gt; &lt;surName&gt;Clark&lt;/surName&gt; &lt;/individualName&gt; &lt;/contact&gt; &lt;/dataset&gt; &lt;/eml:eml&gt; 17.2.3 Validation Errors One of the hardest parts about creating EML in R is diagnosing validation errors. I won’t get into too much detail, but here is a simple example. The eml$... family of helpers can help prevent validation errors, since they have a set list of arguments which are allowed. Here, I bypass the eml$dataset() helper function to show what the error looks like. doc &lt;- list(packageId = &quot;dataset-1&quot;, system = &quot;local&quot;, dataset = list(title = &quot;A minimial valid EML dataset&quot;, creator = eml$creator(individualName = me), contact = eml$contact(individualName = me), arbitrary = &quot;This element isn&#39;t in the schema&quot;)) eml_validate(doc) ## [1] FALSE ## attr(,&quot;errors&quot;) ## [1] &quot;Element &#39;arbitrary&#39;: This element is not expected. Expected is one of ( references, alternateIdentifier, shortName, title ).&quot; This error essentially says that the element arbitrary is not expected, and gives you a hint of some elements that were expected. Validation errors can be tricky, especially when there are lots of them. Validate early and often! 17.2.4 Metadata in R: A more complete example As you might imagine, creating a complete metadata record like what is shown on this page would be pretty tedious if we did it just using the generic list or eml$... helpers, since the nesting structure can be very deep. The EML package has a set of higher level helper functions beginning with set_ that create some of the more complex elements. To demonstrate the use of these we are going to create an EML document that contains the following information: title creator and contact abstract methods geographic and temporal coverage description of a tabular data file and a script We will edit these elements using a mix of helpers and generic techniques. To get set up, navigate to this dataset and download the CSV file and the R script. Put them in a directory called files that is a sub-directory of the location of this RMarkdown file. 17.2.4.1 Title, creator, contact To start, lets create a basic EML skeleton using our example above, but with more information in the creator and contact information besides just my name. # eml creator and contact have identical schema requirements (both fall under `responsibleParty`) me &lt;- eml$creator(individualName = eml$individualName(givenName = &quot;Jeanette&quot;, surName = &quot;Clark&quot;), organizationName = &quot;National Center for Ecological Analysis and Synthesis&quot;, electronicMailAddress = &quot;jclark@nceas.ucsb.edu&quot;, userId = list(directory = &quot;https://orcid.org&quot;, userId = &quot;https://orcid.org/0000-0003-4703-1974&quot;)) doc &lt;- list(packageId = &quot;dataset-1&quot;, system = &quot;local&quot;, dataset = eml$dataset(title = &quot;A more robust valid EML dataset&quot;, creator = me, contact = me)) Because we have used the eml$dataset helper, all of the possible sub-elements have been populated in our EML document, allowing us to easily access and edit them using R autocomplete. 17.2.4.2 Abstract We can use this to dive down into sub-elements and edit them. Let’s do the abstract. This is a simple element so we can just assign the value of the abstract to a character string. doc$dataset$abstract &lt;- &quot;A brief but comprehensive description of the who, what, where, when, why, and how of my dataset.&quot; 17.2.4.3 Methods We can use the set_methods function to parse a markdown (or word) document and insert it into the methods section. This way of adding text is especially nice because it preserves formatting. doc$dataset$methods &lt;- set_methods(&quot;files/methods.md&quot;) doc$dataset$methods ## $sampling ## NULL ## ## $methodStep ## $methodStep$instrumentation ## character(0) ## ## $methodStep$software ## NULL ## ## $methodStep$description ## $methodStep$description$section ## $methodStep$description$section[[1]] ## [1] &quot;&lt;title&gt;Data Collection&lt;/title&gt;\\n&lt;para&gt;\\n We collected some data and here is a description\\n &lt;/para&gt;&quot; ## ## $methodStep$description$section[[2]] ## [1] &quot;&lt;title&gt;Data processing&lt;/title&gt;\\n&lt;para&gt;\\n Here is how we processed the data\\n &lt;/para&gt;&quot; ## ## ## $methodStep$description$para ## list() 17.2.4.4 Coverage The geographic and temporal coverage can be set using set_coverage. doc$dataset$coverage &lt;- set_coverage(beginDate = 2001, endDate = 2010, geographicDescription = &quot;Alaska, United States&quot;, westBoundingCoordinate = -179.9, eastBoundingCoordinate = -120, northBoundingCoordinate = 75, southBoundingCoordinate = 55) 17.2.4.5 Data file: script Information about data files (or entity level information) can be added in child elements of the dataset element. Here we will use the element otherEntity (other options include spatialVector, spatialRaster, and dataTable) to represent the R script. First, some high level information. doc$dataset$otherEntity &lt;- eml$otherEntity(entityName = &quot;files/datfiles_processing.R&quot;, entityDescription = &quot;Data processing script&quot;, entityType = &quot;application/R&quot;) We can use the set_physical helper to set more specific information about the file, like its size, delimiter, and checksum. This function automatically detects fundamental characteristics about the file if you give it a path to your file on your system. doc$dataset$otherEntity$physical &lt;- set_physical(&quot;files/datfiles_processing.R&quot;) ## Automatically calculated file size using file.size(&quot;files/datfiles_processing.R&quot;) ## Automatically calculated authentication size using digest::digest(&quot;files/datfiles_processing.R&quot;, algo = &quot;md5&quot;, file = TRUE) 17.2.4.6 Data file: tabular Here we will use the element dataTable to describe the tabular data file. As before, we set the entityName, entityDescription, and the physical sections. doc$dataset$dataTable &lt;- eml$dataTable(entityName = &quot;files/my-data.csv&quot;, entityDescription = &quot;Temperature data from in-situ loggers&quot;) doc$dataset$dataTable$physical &lt;- set_physical(&quot;files/my-data.csv&quot;) ## Automatically calculated file size using file.size(&quot;files/my-data.csv&quot;) ## Automatically calculated authentication size using digest::digest(&quot;files/my-data.csv&quot;, algo = &quot;md5&quot;, file = TRUE) Next, perhaps the most important part of metadata, but frequently the most difficult to document in a metadata standard: attribute level information. An attribute is a variable in your dataset. For tabular data, this is information about columns within data tables, critical to understanding what kind of information is actually in the table! The set_attributes function will take a data.frame that gives required attribute information. This data.frame contains rows corresponding to column names (or attributes) in the dataset, and columns: attributeName (any text) attributeDefinition (any text) unit (required for numeric data, use get_unitList() to see a list of standard units) numberType (required for numeric data, one of: real, natural, whole, integer) formatString (required for dateTime data) definition (required for textDomain data) Two other sub-elements, the domain and measurementScale, can be inferred from the col_classes argument to set_attributes. Let’s create our attributes data.frame. atts &lt;- data.frame(attributeName = c(&quot;time&quot;, &quot;temperature&quot;, &quot;site&quot;), attributeDefinition = c(&quot;time of measurement&quot;, &quot;measured temperature in degrees Celsius&quot;, &quot;site identifier&quot;), unit = c(NA, &quot;celsius&quot;, NA), numberType = c(NA, &quot;real&quot;, NA), formatString = c(&quot;HH:MM:SS&quot;, NA, NA), definition = c(NA, NA, &quot;site identifier&quot;)) We will then use this in our set_attributes function, along with the col_classes argument, to generate a complete attributeList. doc$dataset$dataTable$attributeList &lt;- set_attributes(attributes = atts, col_classes = c(&quot;Date&quot;, &quot;numeric&quot;, &quot;character&quot;)) As you might imagine, this can get VERY tedious with wide data tables. The function shiny_attributes calls an interactive table that can not only automatically detect and attempt to fill in attribute information from a data.frame, but also helps with on the fly validation. Note: this requires that the shinyjs package is installed. atts_shiny &lt;- shiny_attributes(data = read.csv(&quot;files/my-data.csv&quot;)) This produces a data.frame that you can insert into set_attributes as above. 17.2.4.7 Validating and writing the file Finally, we need to validate and write our file. eml_validate(doc) ## [1] TRUE ## attr(,&quot;errors&quot;) ## character(0) write_eml(doc, &quot;files/complex_example.xml&quot;) 17.3 Publish data to the Arctic Data Center test site 17.3.1 Setup and Introduction Now let’s see how to use the dataone and datapack R packages to upload data to DataONE member repositories like the KNB Data Repository and the Arctic Data Center. 17.3.1.1 The dataone package The dataone R package provides methods to enable R scripts to interact with DataONE to search for, download, upload and update data and metadata. The purpose of uploading data from R is to automate the repetitive tasks for large datasets with many files. For small datasets, the web submission form will certainly be simpler. The dataone package interacts with two major parts of the DataONE infrastructure: Coordinating Nodes (or cn) and Member Nodes (or mn). Coordinating nodes maintain a complete catalog of all data and provide the core DataONE services, including search and discovery. The cn that is exposed through search.dataone.org is called the production (or PROD) cn. Member Nodes expose their data and metadata through a common set of interfaces and services. The Arctic Data Center is an mn. To post data to the Arctic Data Center, we need to interact with both the coordinating and member nodes. In addition to the production cn, there are also several test coordinating node environments, and corresponding test member node environments. In this tutorial, we will be posting data to the test Arctic Data Center environment. 17.3.1.2 The datapack package The datapack R package represents the set of files in a dataset as a datapack::DataPackage. This DataPackage is just a special R object class that is specified in the datapack package. Each object in that DataPackage is represented as a DataObject, another R object class specified by datapack. When you are publishing your dataset, ideally you aren’t only publishing a set of observations. There are many other artifacts of your research, such as scripts, derived data files, and derived figures, that should be archived in a data package. Each of the types of files shown in the workflow below should be considered a data object in your package, including the metadata file that describes the individual components of your workflow. Each of the files in the diagram above has a relationship with the other files, and using datapack you can describe these relationships explicitly and unambiguously using controlled vocabularies and conceptual frameworks. For example, we know that the fine “Image 1” was generated by “Mapping Script.” We also know that both “Image 1” and “Mapping Script” are described by the file “Metadata.” Both of these relationsips are represented in datapack using speficic ontologies. Using datapack we will create a DataPackage that represents a (very simple) scientific workflow and the data objects within it, where the relationships between those objects are explicitly described. Then, we will upload this DataPackage to a test version of the Arctic Data Center repository using dataone. Before uploading any data to a DataONE repository, you must login to get an authentication token, which is a character string used to identify yourself. This token can be retrieved by logging into the test repository and copying the token into your R session. 17.3.1.3 Obtain an ORCID ORCID is a researcher identifier that provides a common way to link your researcher identity to your articles and data. An ORCID is to a researcher as a DOI is to a research article. To obtain an ORCID, register at https://orcid.org. 17.3.1.4 Log in to the test repository and copy your token We will be using a test server, so login and retrieve your token at https://test.arcticdata.io. Once you are logged in, navigate to your Profile Settings, and locate the “Authentication Token” section, and then copy the token for R to your clipboard. Finally, paste the token into the R Console to register it as an option for this R session. You are now logged in. But note that you need to keep this token private; don’t paste it into scripts or check it into git, as it is just as sensitive as your password. 17.3.2 Uploading A Package Using R with uploadDataPackage Datasets and metadata can be uploaded individually or as a collection. Such a collection, whether contained in local R objects or existing on a DataONE repository, will be informally referred to as a package. Load the libraries: library(dataone) library(datapack) First we need to create an R object that describes what coordinating and member nodes we will be uploading our dataset to. We do this with the dataone function D1Client (DataONE client). The first argument specifies the DataONE coordinating node (in this case a test node called STAGING) and the second specifices the member node. We’ll also create an object that only represents the member node, which is helpful for some functions. d1c &lt;- D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) mn &lt;- d1c@mn Next, we create a DataPackage as a container for our data, metadata, and scripts using the new function. This just creates an empty object with the class DataPackage dp &lt;- new(&quot;DataPackage&quot;) dp ## This package does not contain any DataObjects. We then need to add a metadata file and data file to this package. First we generate some identifiers for the objects. We’ll use the uuid scheme for all of our objects. If we were uploading to production, you would likely want use an identifier with the doi scheme for your metadata document. data_id &lt;- generateIdentifier(mn, scheme = &quot;uuid&quot;) script_id &lt;- generateIdentifier(mn, scheme = &quot;uuid&quot;) metadata_id &lt;- generateIdentifier(mn, scheme = &quot;uuid&quot;) Now we need to modify our EML document to include these identifiers. This increases the accessibility of the files in our dataset. First read in the EML that we created earlier. doc &lt;- read_eml(&quot;files/complex_example.xml&quot;) Let’s replace the arbitrary packageId and system that we set in the example above to reflect the identifier we created for this package, and the system we are uploading the package to. doc$packageId &lt;- metadata_id doc$system &lt;- mn@identifier Now let’s add a distribution URL to the physical section of our entity information. All of the distribution URLs look something like this https://test.arcticdata.io/metacat/d1/mn/v2/object/urn:uuid:A0cc95b46-a318-4dd1-8f8a-0e6c280e1d95, where https://test.arcticdata.io/metacat/d1/mn/v2/ is the member node end point, and urn:uuid:A0cc95b46-a318-4dd1-8f8a-0e6c280e1d95 is the identifier of the object. We can easily construct this URL using the paste0 function, and insert it into the physical section of the dataTable element. # set url for csv doc$dataset$dataTable$physical$distribution$online$url &lt;- paste0(mn@endpoint, &quot;object/&quot;, data_id) # set url for script doc$dataset$otherEntity$physical$distribution$online$url &lt;- paste0(mn@endpoint, &quot;object/&quot;, script_id) write_eml(doc, &quot;files/complex_example.xml&quot;) Now we have a full metadata document ready to be uploaded. We now need to add our files to the DataPackage. First, let’s create a new DataObject, which is another object class specific to datapack. Our metadata file, data file, and script will all need to be added as a DataObject. Remember that all files in a package are considered data objects, not just the ones that we you traditionally think of as being “data”. The format argument specifies the type of file, and should match one of the list of DataONE formatIds (listed in the Id field) here. # Add the metadata document to the package metadataObj &lt;- new(&quot;DataObject&quot;, id = metadata_id, format =&quot;eml://ecoinformatics.org/eml-2.1.1&quot;, filename = &quot;files/complex_example.xml&quot;) After creating the DataObject that represents the metadata file, we add it to the data package using addMember. dp &lt;- addMember(dp, metadataObj) dp ## Members: ## ## filename format mediaType size identifier modified local ## comple...le.xml eml...1.1 NA 5841 urn:u...7ffd1 n y ## ## Package identifier: NA ## RightsHolder: NA ## ## ## This package does not contain any provenance relationships. We add the data file and the script similarly. The only difference is in the addMember function we have to set the mo (metadata object) argument equal to the DataObject that represents the metadata file. Adding our csv file to the package this way not only adds the file to the data package, but it also specifies that the csv is described by the metadata. # Add our data file to the package sourceObj &lt;- new(&quot;DataObject&quot;, id = data_id, format = &quot;text/csv&quot;, filename = &quot;files/my-data.csv&quot;) dp &lt;- addMember(dp, sourceObj, mo = metadataObj) dp ## Members: ## ## filename format mediaType size identifier modified local ## my-data.csv text/csv NA 45 urn:u...4a4aa n y ## comple...le.xml eml...1.1 NA 5841 urn:u...7ffd1 n y ## ## Package identifier: NA ## RightsHolder: NA ## ## ## Relationships (updated): ## ## subject predicate object ## 1 complex_example.xml cito:documents my-data.csv ## 2 my-data.csv cito:isDocumentedBy complex_example.xml Next we add our script in the same way. # Add our script to the package scriptObj &lt;- new(&quot;DataObject&quot;, id = script_id, format = &quot;application/R&quot;, filename = &quot;files/datfiles_processing.R&quot;) dp &lt;- addMember(dp, scriptObj, mo = metadataObj) dp ## Members: ## ## filename format mediaType size identifier modified local ## my-data.csv text/csv NA 45 urn:u...4a4aa n y ## comple...le.xml eml...1.1 NA 5841 urn:u...7ffd1 n y ## datfil...sing.R app...n/R NA 5625 urn:u...38ed9 n y ## ## Package identifier: NA ## RightsHolder: NA ## ## ## Relationships (updated): ## ## subject predicate object ## 1 complex_example.xml cito:documents my-data.csv ## 3 complex_example.xml cito:documents datfiles_processing.R ## 4 datfiles_processing.R cito:isDocumentedBy complex_example.xml ## 2 my-data.csv cito:isDocumentedBy complex_example.xml You can also specify other information about the relationships between files in your data package by adding provenance information. Here, we specify that the R script (program) uses the csv (sources) by including them as the specified arguments in the describeWorkflow function. dp &lt;- describeWorkflow(dp, sources = sourceObj, program = scriptObj) dp ## Members: ## ## filename format mediaType size identifier modified local ## my-data.csv text/csv NA 45 urn:u...4a4aa n y ## comple...le.xml eml...1.1 NA 5841 urn:u...7ffd1 n y ## datfil...sing.R app...n/R NA 5625 urn:u...38ed9 n y ## ## Package identifier: NA ## RightsHolder: NA ## ## ## Relationships (updated): ## ## subject predicate object ## 9 _e193edd0...9ae871047 rdf:type prov:Association ## 8 _e193edd0...9ae871047 prov:hadPlan datfiles_processing.R ## 1 complex_example.xml cito:documents my-data.csv ## 3 complex_example.xml cito:documents datfiles_processing.R ## 4 datfiles_processing.R cito:isDocumentedBy complex_example.xml ## 10 datfiles_processing.R rdf:type provone:Program ## 2 my-data.csv cito:isDocumentedBy complex_example.xml ## 5 my-data.csv rdf:type provone:Data ## 11 urn:uuid:...c4ff5b5fb dcterms:identifier urn:uuid:...c4ff5b5fb ## 7 urn:uuid:...c4ff5b5fb rdf:type provone:Execution ## 6 urn:uuid:...c4ff5b5fb prov:qual...sociation _e193edd0...9ae871047 ## 12 urn:uuid:...c4ff5b5fb prov:used my-data.csv Each object in a data package has an access policy. There are three levels of access you can grant either to individual files, or the package as a whole. read: the ability to view when published write: the ablity to edit after publishing changePermission: the ability to grant other people read, write, or changePermission access Here, I give my colleague (via his ORCID) “read” and “write” permission to my entire data package using addAccessRule. dp &lt;- addAccessRule(dp, subject = &quot;http://orcid.org/0000-0003-0077-4738&quot;, permission = c(&quot;read&quot;,&quot;write&quot;), getIdentifiers(dp)) Finally, we upload the package to the test server for the Arctic Data Center using uploadDataPackage. This function takes as arguments d1c (the DataONE client which specifies which coordinating and member nodes to use), dp (the data package itself), whether you want the package to include public read access (public = TRUE). packageId &lt;- uploadDataPackage(d1c, dp, public = TRUE) You can now search for and view the package at https://test.arcticdata.io: "],
["reproducibility-and-provenance.html", "18 Reproducibility and Provenance 18.1 Learning Objectives", " 18 Reproducibility and Provenance 18.1 Learning Objectives In this lesson, you will learn: About the importance of computational reproducibility The role of provenance metadata Tools and techniques for reproducibility supportred by the Arctic Data Center How to build a reproducible paper in RMarkdown A great overview of this approach to reproducible papers comes from: Ben Marwick, Carl Boettiger &amp; Lincoln Mullen (2018) Packaging Data Analytical Work Reproducibly Using R (and Friends), The American Statistician, 72:1, 80-88, doi:10.1080/00031305.2017.1375986 This lesson will draw from existing materials: Accelerating synthesis science through reproducible science practices rrtools Reproducible papers with RMarkdown To start a reproducible paper with rrtools, run: devtools::install_github(&quot;benmarwick/rrtools&quot;) setwd(&quot;..&quot;) rrtools::use_compendium(&quot;arcticpaper&quot;) Then, add some more structure to the package: usethis::use_apl2_license(name=&quot;Matthew B. Jones&quot;) rrtools::use_readme_rmd() rrtools::use_analysis() Now write a reproducible paper! "],
["references-1.html", "19 References", " 19 References "]
]
